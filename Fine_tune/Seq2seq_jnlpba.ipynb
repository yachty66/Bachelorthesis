{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text infilling: context, T, attribute, A, and the value, V , in a sentence as “T. A is V .” where the attribute value V is masked as blank\n",
    "\n",
    "Answer generation: generate V as the answer, considering T as the context and A as the question.\n",
    "\n",
    "Answer generation seems to be plausible. I just train the a model on that. I will just use T5 because its not important for me to have the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import(\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cf02d3f6b14a12b4b22980c0acef27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'tokens': ['IL-2', 'gene', 'expression', 'and', 'NF-kappa', 'B', 'activation', 'through', 'CD28', 'requires', 'reactive', 'oxygen', 'production', 'by', '5-lipoxygenase', '.'], 'ner_tags': [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "jnlpba = load_dataset('jnlpba', split=['train', \"validation\"])\n",
    "print(jnlpba[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a df with question, context and answer like:\n",
    "\n",
    "What are the entities?\n",
    "\n",
    "IL-2', 'gene', 'expression', 'and', 'NF-kappa', 'B', 'activation', 'through', 'CD28', 'requires', 'reactive', 'oxygen', 'production','by', '5-lipoxygenase', '.\n",
    "\n",
    "attr1, attr2, attr3\n",
    "\n",
    "The question I still have is how i need to tokenize everything - how i am going to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                             tokens  \\\n",
      "0          1  [IL-2, gene, expression, and, NF-kappa, B, act...   \n",
      "1          2  [Activation, of, the, CD28, surface, receptor,...   \n",
      "2          3  [In, primary, T, lymphocytes, we, show, that, ...   \n",
      "3          4  [Delineation, of, the, CD28, signaling, cascad...   \n",
      "4          5  [Our, data, suggest, that, lipoxygenase, metab...   \n",
      "...      ...                                                ...   \n",
      "37089  37090  [This, stimulation, can, also, be, abolished, ...   \n",
      "37090  37091  [This, gp160, treatment, adversely, affects, t...   \n",
      "37091  37092  [Effects, similar, to, gp160, were, seen, with...   \n",
      "37092  37093  [The, aberrant, activation, of, AP-1, by, gp16...   \n",
      "37093  37094                                                 []   \n",
      "\n",
      "                                                ner_tags  \n",
      "0      [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]  \n",
      "1      [0, 0, 0, 9, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "2      [0, 7, 8, 8, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3      [0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 9, 10, 10, 0, 0...  \n",
      "4      [0, 0, 0, 0, 9, 10, 0, 0, 0, 0, 0, 0, 9, 0, 0,...  \n",
      "...                                                  ...  \n",
      "37089  [0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 10, 10, 0, 0, 0...  \n",
      "37090  [0, 9, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 7, ...  \n",
      "37091                    [0, 0, 0, 9, 0, 0, 0, 9, 10, 0]  \n",
      "37092  [0, 0, 0, 0, 9, 0, 9, 0, 7, 8, 8, 8, 0, 0, 0, ...  \n",
      "37093                                                 []  \n",
      "\n",
      "[37094 rows x 3 columns]\n",
      "37094\n",
      "37094\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1549\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1549\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selected_obj)\n\u001b[1;32m   1550\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m     \u001b[39m# gh-20949\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m     \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m     \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# on a string grouper column\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1601\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[39mApply function f in python space\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[39m    data after applying f\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m values, mutated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mapply(f, data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n\u001b[1;32m   1602\u001b[0m \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/groupby/ops.py:839\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    838\u001b[0m group_axes \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39maxes\n\u001b[0;32m--> 839\u001b[0m res \u001b[39m=\u001b[39m f(group)\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mutated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "Cell \u001b[0;32mIn [8], line 37\u001b[0m, in \u001b[0;36mcombine_rows\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_rows\u001b[39m(group):\n\u001b[1;32m     35\u001b[0m     new_row \u001b[39m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: group[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m],\n\u001b[0;32m---> 37\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(group[\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     38\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(group[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     39\u001b[0m     }\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mSeries(new_row)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 42\u001b[0m\n\u001b[1;32m     35\u001b[0m     new_row \u001b[39m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: group[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m],\n\u001b[1;32m     37\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(group[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m     38\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(group[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     39\u001b[0m     }\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mSeries(new_row)\n\u001b[0;32m---> 42\u001b[0m df_train \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mapply(combine_rows)\n\u001b[1;32m     43\u001b[0m df_val \u001b[39m=\u001b[39m df_val\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mapply(combine_rows)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(df_train))\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1560\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m         \u001b[39m# gh-20949\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m         \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m         \u001b[39m# on a string grouper column\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_selection_context():\n\u001b[0;32m-> 1560\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selected_obj)\n\u001b[1;32m   1562\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1601\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   1565\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_python_apply_general\u001b[39m(\n\u001b[1;32m   1566\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     is_agg: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1572\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[1;32m   1573\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[39m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[39m        data after applying f\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m     values, mutated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mapply(f, data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1603\u001b[0m         not_indexed_same \u001b[39m=\u001b[39m mutated \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/groupby/ops.py:839\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39m# group might be modified\u001b[39;00m\n\u001b[1;32m    838\u001b[0m group_axes \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39maxes\n\u001b[0;32m--> 839\u001b[0m res \u001b[39m=\u001b[39m f(group)\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mutated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    841\u001b[0m     mutated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [8], line 36\u001b[0m, in \u001b[0;36mcombine_rows\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_rows\u001b[39m(group):\n\u001b[1;32m     35\u001b[0m     new_row \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 36\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: group[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m],\n\u001b[1;32m     37\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(group[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m     38\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(group[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     39\u001b[0m     }\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mSeries(new_row)\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "#create dataframe with the 3 columns \n",
    "\n",
    "#create function which adds columns to df as i want \n",
    "df_train = pd.DataFrame(jnlpba[0])\n",
    "print(df_train)\n",
    "df_val = pd.DataFrame(jnlpba[1])\n",
    "print(len(df_train))\n",
    "# Create a mapping between the ner_tags values and the attribute values\n",
    "mapping = {\n",
    "    0: \"O\",\n",
    "    1: \"B-DNA\",\n",
    "    2: \"I-DNA\",\n",
    "    3: \"B-RNA\",\n",
    "    4: \"I-RNA\",\n",
    "    5: \"B-cell_line\",\n",
    "    6: \"I-cell_line\",\n",
    "    7: \"B-cell_type\",\n",
    "    8: \"I-cell_type\",\n",
    "    9: \"B-protein\",\n",
    "    10: \"I-protein\"\n",
    "}\n",
    "\n",
    "def replace_tags(row):\n",
    "    row['ner_tags'] = ' '.join([mapping[tag] for tag in row['ner_tags']])\n",
    "    return row\n",
    "\n",
    "def join_text(row):\n",
    "    row['tokens'] = ' '.join(row['tokens'])\n",
    "    return row\n",
    "    \n",
    "df_train = df_train.apply(replace_tags, axis=1)\n",
    "df_val = df_val.apply(replace_tags, axis=1)\n",
    "print(len(df_train))\n",
    "def combine_rows(group):\n",
    "    new_row = {\n",
    "        'id': group['id'].iloc[0],\n",
    "        'tokens': ' '.join(group['tokens']),\n",
    "        'ner_tags': ' '.join(group['ner_tags'])\n",
    "    }\n",
    "    return pd.Series(new_row)\n",
    "\n",
    "df_train = df_train.groupby('id').apply(combine_rows)\n",
    "df_val = df_val.groupby('id').apply(combine_rows)\n",
    "print(len(df_train))\n",
    "#add to data as context the column tokens and as answer the column ner_tags, answer is always the same \"What are the attributes?\"\n",
    "data = {\"context\": df_train['tokens'], \"question\": \"What are the attributes?\", \"answer\": df_train['ner_tags']}\n",
    "\n",
    "\n",
    "\n",
    "# Create the data frame using the pd.DataFrame constructor\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#can i not just use the number representation? The reason why i dont want to do this is because perhaps there is some meaning hidden inside of the bios tag \"gene\" for example.\n",
    "\n",
    "#lets get this fucking bios tag representation - before i would like to fix the gh bug-."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toknization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: T5Tokenizer, answer_max_token_len, question_max_token_len, context_max_token_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.answer_max_token_len = data['context'].apply(len).max()\n",
    "        self.question_max_token_len = data['answer'].apply(len).max()\n",
    "        self.context_max_token_len = data['question'].apply(len).max()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        encoding_context = tokenizer(\n",
    "        data_row[\"context\"].tolist(),\n",
    "        max_length=512,\n",
    "        padding=self.context_max_token_len,\n",
    "        truncation=\"only_second\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        encoding_answer = tokenizer(\n",
    "        data_row[\"answer\"].tolist(),\n",
    "        padding=self.answer_max_token_len,\n",
    "        truncation=\"only_second\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"   \n",
    "        )\n",
    "        \n",
    "        encoding_question = tokenizer(\n",
    "        data_row[\"question\"].tolist(),\n",
    "        padding=self.question_max_token_len,\n",
    "        truncation=\"only_second\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = encoding_answer['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            question = data_row[\"question\"],\n",
    "            context = data_row[\"context\"],\n",
    "            answer = data_row[\"answer\"],\n",
    "            input_ids = encoding_context['input_ids'].flatten(),\n",
    "            attention_mask = encoding_context['attention_mask'].flatten(),\n",
    "            labels = labels.flatten()   \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = NERDataset(df, tokenizer, max_length_answer, max_length_question, max_length_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f68a4539c9a11cef9bf0819cdddedfa00ec9d5fcff3291c5b30fad122c003099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
