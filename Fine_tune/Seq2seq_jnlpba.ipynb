{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text infilling: context, T, attribute, A, and the value, V , in a sentence as “T. A is V .” where the attribute value V is masked as blank\n",
    "\n",
    "Answer generation: generate V as the answer, considering T as the context and A as the question.\n",
    "\n",
    "Answer generation seems to be plausible. I just train the a model on that. I will just use T5 because its not important for me to have the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import(\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d075d5ea53e4dbbaaf22296ef900aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'tokens': ['IL-2', 'gene', 'expression', 'and', 'NF-kappa', 'B', 'activation', 'through', 'CD28', 'requires', 'reactive', 'oxygen', 'production', 'by', '5-lipoxygenase', '.'], 'ner_tags': [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "jnlpba = load_dataset('jnlpba', split=['train[:10]', \"validation[:10]\"])\n",
    "print(jnlpba[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a df with question, context and answer like:\n",
    "\n",
    "What are the entities?\n",
    "\n",
    "IL-2', 'gene', 'expression', 'and', 'NF-kappa', 'B', 'activation', 'through', 'CD28', 'requires', 'reactive', 'oxygen', 'production','by', '5-lipoxygenase', '.\n",
    "\n",
    "attr1, attr2, attr3\n",
    "\n",
    "The question I still have is how i need to tokenize everything - how i am going to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                             tokens  \\\n",
      "0  1  [IL-2, gene, expression, and, NF-kappa, B, act...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]  \n",
      "  id                                             tokens  \\\n",
      "0  1  IL-2 gene expression and NF-kappa B activation...   \n",
      "1  2  Activation of the CD28 surface receptor provid...   \n",
      "2  3  In primary T lymphocytes we show that CD28 lig...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  B-DNA I-DNA O O B-protein I-protein O O B-prot...  \n",
      "1  O O O B-protein I-protein I-protein O O O O O ...  \n",
      "2  O B-cell_type I-cell_type I-cell_type O O O B-...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(jnlpba[0])\n",
    "print(df_train.head(1))\n",
    "df_val = pd.DataFrame(jnlpba[1])\n",
    "\n",
    "mapping = {\n",
    "    0: \"O\",\n",
    "    1: \"B-DNA\",\n",
    "    2: \"I-DNA\",\n",
    "    3: \"B-RNA\",\n",
    "    4: \"I-RNA\",\n",
    "    5: \"B-cell_line\",\n",
    "    6: \"I-cell_line\",\n",
    "    7: \"B-cell_type\",\n",
    "    8: \"I-cell_type\",\n",
    "    9: \"B-protein\",\n",
    "    10: \"I-protein\"\n",
    "}\n",
    "\n",
    "def map_tags(row):\n",
    "    row['ner_tags'] = [' '.join([mapping[tag] for tag in row['ner_tags']])][0]\n",
    "    return row\n",
    "\n",
    "df_train = df_train.apply(map_tags, axis=1)\n",
    "df_val = df_val.apply(map_tags, axis=1)\n",
    "\n",
    "def join_tags(row):\n",
    "    row['tokens'] = ' '.join(row['tokens'])\n",
    "    return row\n",
    "\n",
    "df_train = df_train.apply(join_tags, axis=1)\n",
    "df_val = df_val.apply(join_tags, axis=1)\n",
    "\n",
    "print(df_train.head(3))\n",
    "\n",
    "data_train = {\"context\": df_train['tokens'], \"question\": \"What are the attributes?\", \"answer\": df_train['ner_tags']}\n",
    "data_val = {\"context\": df_val['tokens'], \"question\": \"What are the attributes?\", \"answer\": df_val['ner_tags']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toknization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: T5Tokenizer, answer_max_token_len: int = 0 , question_max_token_len: int=0, context_max_token_len: int=0):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.answer_max_token_len = max(data['context'], key=len)\n",
    "        self.question_max_token_len = max(data['question'], key=len)\n",
    "        self.context_max_token_len = max(data['context'], key=len)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        encoding_context = tokenizer(    \n",
    "            data_row[\"context\"].tolist(),\n",
    "            max_length=self.context_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        encoding_answer = tokenizer(\n",
    "            data_row[\"answer\"].tolist(),\n",
    "            max_length=self.answer_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"   \n",
    "        )\n",
    "        \n",
    "        encoding_question = tokenizer(\n",
    "            data_row[\"question\"].tolist(),\n",
    "            max_length=self.question_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = encoding_answer['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            question = data_row[\"question\"],\n",
    "            context = data_row[\"context\"],\n",
    "            answer = data_row[\"answer\"],\n",
    "            input_ids = encoding_context['input_ids'].flatten(),\n",
    "            attention_mask = encoding_context['attention_mask'].flatten(),\n",
    "            labels = labels.flatten()   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset_train = NERDataset(data_train, tokenizer)\n",
    "sample_dataset_val = NERDataset(data_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a058b126f0d4d3aa5ae14b2fdea6000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m sample_dataset_train:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [117], line 13\u001b[0m, in \u001b[0;36mNERDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m---> 13\u001b[0m     data_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49miloc[index]\n\u001b[1;32m     14\u001b[0m     encoding_context \u001b[39m=\u001b[39m tokenizer(    \n\u001b[1;32m     15\u001b[0m         data_row[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     16\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_max_token_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     encoding_answer \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     25\u001b[0m         data_row[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     26\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswer_max_token_len,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m   \n\u001b[1;32m     32\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "for i in sample_dataset_train:\n",
    "    print(i)\n",
    "    break\n",
    "\n",
    "\n",
    "#next i need to train my model - i can do this direct with huggingface instead with pytorch lightning \n",
    "#first i need to check why my dataset is not converted to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still need to turn the attributes representation into a string and not list done\n",
    "#what is next. nex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f68a4539c9a11cef9bf0819cdddedfa00ec9d5fcff3291c5b30fad122c003099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
