{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import DatasetDict\n",
    "\n",
    "#todo need to change and train on full dataset in final version\n",
    "jnlpba = load_dataset('jnlpba', split=['train[:10]', \"validation[:10]\"])\n",
    "\n",
    "datasets = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})\n",
    "\n",
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def weak(lst):\n",
    "    random_number = 0\n",
    "    item = random_number\n",
    "    item_actual = 0\n",
    "    item_set = 0\n",
    "    while item_actual == item_set:\n",
    "        random_number = random.randint(0, len(lst)-1)\n",
    "        random_number_item = random.randint(0, len(label_list)-1)\n",
    "        item_set = random_number_item\n",
    "        item_actual = lst[random_number]\n",
    "    new_lst = lst.copy()\n",
    "    new_lst[random_number] = item_set\n",
    "    return new_lst\n",
    "df = datasets[\"train\"].to_pandas()\n",
    "df[\"ner_tags\"] = df[\"ner_tags\"].apply(weak)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df)\n",
    "\n",
    "datasets = DatasetDict({\"train\": dataset_train, \"validation\": jnlpba[1]})\n",
    "\n",
    "#i think the problem is that the new dataset which i created is not the same format like the dataset before\n",
    "#can check that in starting training again without weak() method. if this works than I can try to exchange only the rows'''\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "\n",
    "\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}-weak-labelled\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "model,\n",
    "args=args,\n",
    "train_dataset=tokenized_datasets[\"train\"],\n",
    "eval_dataset=tokenized_datasets[\"validation\"],\n",
    "data_collator=data_collator,\n",
    "tokenizer=tokenizer,\n",
    "compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](../../images/weak_ner_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d1df2bf7434216bc988a1245c8cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#there are basically three different options \n",
    "#1. label is missing 2. wrong label 3. Mising inside\n",
    "#in each entry where I make the dataset weak I apply one of this methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4c8a2f6dd249ccbc797f6f615a4cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                             tokens  \\\n",
      "0   1  [IL-2, gene, expression, and, NF-kappa, B, act...   \n",
      "1   2  [Activation, of, the, CD28, surface, receptor,...   \n",
      "2   3  [In, primary, T, lymphocytes, we, show, that, ...   \n",
      "3   4  [Delineation, of, the, CD28, signaling, cascad...   \n",
      "4   5  [Our, data, suggest, that, lipoxygenase, metab...   \n",
      "5   6  [These, findings, should, be, useful, for, the...   \n",
      "6   7  [The, peri-kappa, B, site, mediates, human, im...   \n",
      "7   8  [Human, immunodeficiency, virus, type, 2, (, H...   \n",
      "8   9  [HIV-1, and, HIV-2, display, significant, diff...   \n",
      "9  10  [Consistent, with, these, differences, ,, we, ...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]  \n",
      "1  [0, 0, 0, 9, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
      "2  [0, 7, 8, 8, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 9, 10, 10, 0, 0...  \n",
      "4  [0, 0, 0, 0, 9, 10, 0, 0, 0, 0, 0, 0, 9, 0, 0,...  \n",
      "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6  [0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 0, 0, 7, 0, ...  \n",
      "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import DatasetDict, Dataset\n",
    "jnlpba = load_dataset('jnlpba', split=['train[:10]', \"validation[:10]\"])\n",
    "jnlpba = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})\n",
    "#turn jnlpba into dataframe\n",
    "df = jnlpba[\"train\"].to_pandas()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2ead0c819540ae9f286825d5c0b9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_34595/2487113789.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row[\"ner_tags\"] = lst\n",
      "/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_34595/2487113789.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row[\"ner_tags\"] = lst\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import DatasetDict, Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "jnlpba = load_dataset('jnlpba', split=['train[:3000]', \"validation[:100]\"])\n",
    "jnlpba = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})\n",
    "\n",
    "\n",
    "class JnlpbDataset():\n",
    "    def __init__(self, dataset, portion, type_path):\n",
    "        self.dataset = dataset[type_path]\n",
    "        self.portion = portion\n",
    "        self.merge()\n",
    "        self.apply()\n",
    "        \n",
    "    def map_tags(self, row):\n",
    "        mapping = {\n",
    "            0: \"O\",\n",
    "            1: \"B-DNA\",\n",
    "            2: \"I-DNA\",\n",
    "            3: \"B-RNA\",\n",
    "            4: \"I-RNA\",\n",
    "            5: \"B-cell_line\",\n",
    "            6: \"I-cell_line\",\n",
    "            7: \"B-cell_type\",\n",
    "            8: \"I-cell_type\",\n",
    "            9: \"B-protein\",\n",
    "            10: \"I-protein\"\n",
    "        }\n",
    "        row['ner_tags'] = [[mapping[tag] for tag in row['ner_tags']]][0]\n",
    "        return row        \n",
    "\n",
    "    def merge_tags(self, tags, tokens):\n",
    "      #todo test if this works also in the scenario of having two B- tags side by side\n",
    "      merged_tags = []\n",
    "      merged_tokens = []\n",
    "      i = 0\n",
    "      while i < len(tags):\n",
    "          if tags[i].startswith('B-'):\n",
    "              merged_tag = tags[i][2:]\n",
    "              merged_token = tokens[i]\n",
    "              i += 1\n",
    "              while i < len(tags) and tags[i].startswith('I-'):\n",
    "                  merged_tag += ' ' + tags[i][2:]\n",
    "                  merged_token += ' ' + tokens[i]\n",
    "                  i += 1\n",
    "              merged_tags.append(merged_tag)\n",
    "              merged_tokens.append(merged_token)\n",
    "          else:\n",
    "              merged_tags.append(tags[i])\n",
    "              merged_tokens.append(tokens[i])\n",
    "              i += 1\n",
    "      for i in range(len(merged_tags)):\n",
    "        s = merged_tags[i].split()[0]\n",
    "        s = s[0].upper() + s[1:]\n",
    "        merged_tags[i] = s\n",
    "      return merged_tags, merged_tokens\n",
    "\n",
    "    def merge(self):\n",
    "      df_train = pd.DataFrame(self.dataset)\n",
    "      df_train = df_train.apply(self.map_tags, axis=1)\n",
    "      df_train[['ner_tags', 'tokens']] = df_train.apply(lambda x: self.merge_tags(x['ner_tags'], x['tokens']), axis=1, result_type='expand')\n",
    "      self.dataset = Dataset.from_pandas(df_train)\n",
    "        \n",
    "    def missing(self, row):\n",
    "        #print(len(self.label_list))\n",
    "        lst = row[\"ner_tags\"]\n",
    "        if any(x != 0 for x in lst):\n",
    "            index = random.choice([i for i, x in enumerate(lst) if x != 0])\n",
    "            lst[index] = 0\n",
    "            row[\"ner_tags\"] = lst\n",
    "            return row\n",
    "        else:\n",
    "            return row\n",
    "\n",
    "    def wrong(self, row, num_tags):\n",
    "        lst = row[\"ner_tags\"]\n",
    "        tags = []\n",
    "        #it would be nice to have a debugger who can display me the values of the vars\n",
    "        for i in range(1,num_tags):\n",
    "            tags.append(i)\n",
    "        if any(x != 0 for x in lst):    \n",
    "            indices = [i for i, x in enumerate(lst) if x != 0]\n",
    "            random_index = random.choice(indices)\n",
    "            current_value = lst[random_index]\n",
    "            random_number = random.choice([x for x in [1, 2, 3, 4, 5] if x != current_value])\n",
    "            lst[random_index] = random_number\n",
    "            row[\"ner_tags\"] = lst\n",
    "            return row\n",
    "        else:\n",
    "            return row\n",
    "                        \n",
    "    def uncomplete(self):\n",
    "        #todo should i implement this\n",
    "        pass\n",
    "\n",
    "    def apply(self):\n",
    "        shuffled_data = self.dataset.shuffle()\n",
    "        num_portion = int(len(self.dataset) * self.portion / 100)\n",
    "        df = self.dataset.to_pandas() \n",
    "        tags = [tag for row in df['ner_tags'] for tag in row]\n",
    "        unique_tags = set(tags)\n",
    "        mapping = {}\n",
    "\n",
    "        for index, item in enumerate(unique_tags):\n",
    "            mapping[item] = index        \n",
    "        #iter over df ner tags column and set each string to number from mapping\n",
    "        df['ner_tags'] = [[mapping[tag] for tag in tags] for tags in df['ner_tags']]    \n",
    "        \n",
    "        for i in range(num_portion):\n",
    "            random_number = random.randint(1, 2)\n",
    "            if random_number == 1:\n",
    "                new_row = self.missing(df.iloc[i])\n",
    "                df.iloc[i] = new_row\n",
    "            elif random_number == 2:\n",
    "                num_tags = len(unique_tags)\n",
    "                new_row = self.wrong(df.iloc[i], num_tags)\n",
    "                df.iloc[i] = new_row\n",
    "            '''else:\n",
    "                self.uncomplete()'''\n",
    "        self.dataset = Dataset.from_pandas(df)\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "        \n",
    "input_dataset = JnlpbDataset(dataset=jnlpba, portion=20, type_path='train')\n",
    "dataset = input_dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import DatasetDict\n",
    "\n",
    "#todo need to change and train on full dataset in final version\n",
    "jnlpba = load_dataset('jnlpba', split=['train[:10]', \"validation[:10]\"])\n",
    "\n",
    "datasets = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})\n",
    "\n",
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def weak(lst):\n",
    "    random_number = 0\n",
    "    item = random_number\n",
    "    item_actual = 0\n",
    "    item_set = 0\n",
    "    while item_actual == item_set:\n",
    "        random_number = random.randint(0, len(lst)-1)\n",
    "        random_number_item = random.randint(0, len(label_list)-1)\n",
    "        item_set = random_number_item\n",
    "        item_actual = lst[random_number]\n",
    "    new_lst = lst.copy()\n",
    "    new_lst[random_number] = item_set\n",
    "    return new_lst\n",
    "df = datasets[\"train\"].to_pandas()\n",
    "df[\"ner_tags\"] = df[\"ner_tags\"].apply(weak)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df)\n",
    "\n",
    "datasets = DatasetDict({\"train\": dataset_train, \"validation\": jnlpba[1]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f68a4539c9a11cef9bf0819cdddedfa00ec9d5fcff3291c5b30fad122c003099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
