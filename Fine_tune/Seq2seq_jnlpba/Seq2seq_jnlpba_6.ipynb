{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/maxhager/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","GPU available: True (mps), used: False\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n","  rank_zero_warn(\n","/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:92: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n","  rank_zero_warn(\n","/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","  | Name  | Type                       | Params\n","-----------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 60.5 M\n","-----------------------------------------------------\n","60.5 M    Trainable params\n","0         Non-trainable params\n","60.5 M    Total params\n","242.026   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"208ccee420bf46cda2dce3bbdbeda9fd","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3211c42fc61f486bbf054c867aff23a9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.true.extend(np.array(true_mapped).flatten())\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:257: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.pred.extend(np.array(pred_mapped).flatten())\n"]},{"name":"stdout","output_type":"stream","text":["4\n","10\n"]},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","  warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["4\n","10\n","val_epoch: 20\n"]},{"name":"stderr","output_type":"stream","text":["Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb942f307b534be890d45aea51554137","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4602248230d4869ab6f08ccf4783a94","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","4\n","10\n","4\n","10\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de676a080bab43f8be72d5b87d43644f","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.true.extend(np.array(true_mapped).flatten())\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:257: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.pred.extend(np.array(pred_mapped).flatten())\n"]},{"name":"stdout","output_type":"stream","text":["4\n","10\n","4\n","10\n","2\n","10\n","val_epoch: 50\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","4\n","10\n","4\n","10\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da255dc2741d4603bc9ff9ac752293d1","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.true.extend(np.array(true_mapped).flatten())\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:257: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.pred.extend(np.array(pred_mapped).flatten())\n"]},{"name":"stdout","output_type":"stream","text":["4\n","10\n","4\n","10\n","2\n","10\n","val_epoch: 80\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","4\n","10\n","4\n","10\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"178c7aa5b1d448d2800b278d4f853f02","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.true.extend(np.array(true_mapped).flatten())\n","/var/folders/r9/30wn6mvs2md_xllcvl5lpn0m0000gn/T/ipykernel_13499/3210327231.py:257: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.pred.extend(np.array(pred_mapped).flatten())\n"]},{"name":"stdout","output_type":"stream","text":["4\n","10\n","4\n","10\n","2\n","10\n","val_epoch: 110\n"]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=3` reached.\n"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["if __name__ == \"__main__\":\n","    from transformers import (\n","        AdamW,\n","        MT5ForConditionalGeneration,\n","        T5ForConditionalGeneration,\n","        T5Tokenizer,\n","        AutoTokenizer,\n","        get_linear_schedule_with_warmup,\n","    )\n","    from datasets import load_dataset, load_metric\n","    from datasets import DatasetDict\n","    import pytorch_lightning as pl\n","    from torch.utils.data import Dataset, DataLoader\n","    import torch\n","    import numpy as np\n","    import pandas as pd\n","    from nltk.tokenize import sent_tokenize\n","    import argparse\n","    import os\n","    import logging\n","    import random\n","    import re\n","    from itertools import chain\n","    from string import punctuation\n","\n","    # import wandb\n","    # from wandb import AlertLevel\n","    from pytorch_lightning import Trainer\n","\n","    # from pytorch_lightning.loggers import WandbLogger\n","    from datasets import load_dataset, load_metric\n","    from datasets import DatasetDict, Dataset\n","    import random\n","    import pandas as pd\n","    import nltk\n","    from dataset import JnlpbDataset\n","\n","    nltk.download(\"punkt\")\n","    random.seed(42)\n","\n","    # wandb.init(project=\"Bachelor_Thesis\", entity=\"maxhager28\", name=\"Seq2seq_jnlpba_strong_test_100\")\n","\n","    def set_seed(seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","    set_seed(42)\n","\n","    from sklearn.metrics import precision_recall_fscore_support\n","    from sklearn.metrics import accuracy_score\n","    import matplotlib.pyplot as plt\n","    from sklearn.metrics import confusion_matrix\n","    import numpy as np\n","\n","    class T5FineTuner(pl.LightningModule):\n","        def __init__(self, hparam):\n","            super(T5FineTuner, self).__init__()\n","            self.hparam = hparam\n","            self.model = T5ForConditionalGeneration.from_pretrained(\n","                hparam.model_name_or_path\n","            )\n","            self.tokenizer = AutoTokenizer.from_pretrained(hparam.model_name_or_path)\n","            self.save_hyperparameters()\n","            self.true = []\n","            self.pred = []\n","            self.counter = 0\n","\n","        def is_logger(self):\n","            return True\n","\n","        def label_true(self, incoming, actual):\n","            #print(\"incoming:\")\n","            #print(len(incoming))\n","            #print(30 * \"_\")\n","            l_targets = [\n","                [tuple_list[0] for tuple_list in sublist] for sublist in actual\n","            ]\n","            #print(\"target:\")\n","            #print(len(l_targets))\n","            #print(30 * \"_\")\n","            l_predictions = [\n","                [\n","                    {e.split(\":\")[0].strip(): e.split(\":\")[1].strip()}\n","                    for e in x.split(\";\")\n","                    if e\n","                ]\n","                for x in incoming\n","            ]\n","            result = []\n","            for inner_list in l_targets:\n","                outcome_inner = []\n","                for word in inner_list:\n","                    found = False\n","                    for dict_list in l_predictions:\n","                        for dict_item in dict_list:\n","                            if word.lower() in dict_item.values():\n","                                outcome_inner.append(list(dict_item.keys())[0])\n","                                found = True\n","                                break\n","                        if found:\n","                            break\n","                    if not found:\n","                        outcome_inner.append(\"O\")\n","                result.append(outcome_inner)\n","            #print(\"result:\")\n","            #print(len(result))\n","            #print(30 * \"_\")\n","            return result\n","\n","        def label_pred(self, incoming, actual):\n","            l_targets = [\n","                [tuple_list[0] for tuple_list in sublist] for sublist in actual\n","            ]\n","            l_predictions = []\n","            for string in incoming:\n","                matches = [\n","                    match\n","                    for match in re.findall(\n","                        r\"(rna: (.+?))(;|$)|(dna: (.+?))(;|$)|(cell_line: (.+?))(;|$)|(protein: (.+?))(;|$)|(cell_type: (.+?))(;|$)\",\n","                        string,\n","                    )\n","                    if match[1] or match[4] or match[7] or match[10] or match[13]\n","                ]\n","                inner_list = []\n","                for match in matches:\n","                    if match[1]:\n","                        inner_list.append({\"rna\": match[1]})\n","                    if match[4]:\n","                        inner_list.append({\"dna\": match[4]})\n","                    if match[7]:\n","                        inner_list.append({\"cell_line\": match[7]})\n","                    if match[10]:\n","                        inner_list.append({\"protein\": match[10]})\n","                    if match[13]:\n","                        inner_list.append({\"cell_type\": match[13]})\n","                l_predictions.append(inner_list)\n","\n","            result = []\n","            for inner_list in l_targets:\n","                outcome_inner = []\n","                for word in inner_list:\n","                    found = False\n","                    for dict_list in l_predictions:\n","                        for dict_item in dict_list:\n","                            if word.lower() in dict_item.values():\n","                                outcome_inner.append(list(dict_item.keys())[0])\n","                                found = True\n","                                break\n","                        if found:\n","                            break\n","                    if not found:\n","                        outcome_inner.append(\"O\")\n","                result.append(outcome_inner)\n","            return result\n","\n","        def forward(\n","            self,\n","            input_ids,\n","            attention_mask=None,\n","            decoder_input_ids=None,\n","            decoder_attention_mask=None,\n","            lm_labels=None,\n","        ):\n","            return self.model(\n","                input_ids,\n","                attention_mask=attention_mask,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_attention_mask=decoder_attention_mask,\n","                labels=lm_labels,\n","            )\n","\n","        def _step(self, batch):\n","\n","            print(len(batch[\"target_ids\"]))\n","            print(len(batch[\"tokens\"]))\n","            \n","            lm_labels = batch[\"target_ids\"]\n","            lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","            outputs = self(\n","                input_ids=batch[\"source_ids\"],\n","                attention_mask=batch[\"source_mask\"],\n","                lm_labels=lm_labels,\n","                decoder_attention_mask=batch[\"target_mask\"],\n","            )\n","            loss = outputs[0]\n","            return loss\n","\n","        def training_step(self, batch, batch_idx):\n","            loss = self._step(batch)\n","            self.log(\"loss\", loss)\n","            return {\"loss\": loss}\n","\n","        def training_epoch_end(self, outputs):\n","            avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","            tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","            # wandb.log({\"avg_train_loss\": avg_train_loss})\n","\n","        def map_tags(self, lst):\n","            mapping = {\n","                \"O\": 0,\n","                \"rna\": 1,\n","                \"dna\": 2,\n","                \"cell_line\": 3,\n","                \"cell_type\": 4,\n","                \"protein\": 5,\n","            }\n","            result = [[mapping[tag] for tag in tags] for tags in lst]\n","            return result\n","\n","        def validation_step(self, batch, batch_idx):\n","            #print(len(batch[\"source_ids\"]))\n","            #print(len(batch[\"tokens\"]))\n","            \n","            outputs = []\n","            targets = []\n","            all_text = []\n","            true_labels = []\n","            pred_labels = []\n","            predictions = []\n","            predictions_temp = []\n","            l_true_labels = []\n","            l_pred_labels = []\n","            input_ids = batch[\"source_ids\"].to(\"cpu\")\n","            attention_mask = batch[\"source_mask\"].to(\"cpu\")\n","            outs = model.model.generate(\n","                input_ids=input_ids, attention_mask=attention_mask\n","            )\n","\n","            dec = [\n","                tokenizer.decode(\n","                    ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n","                ).strip()\n","                for ids in outs\n","            ]\n","            target = [\n","                tokenizer.decode(\n","                    ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n","                ).strip()\n","                for ids in batch[\"target_ids\"]\n","            ]\n","\n","            texts = [\n","                tokenizer.decode(\n","                    ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n","                ).strip()\n","                for ids in batch[\"source_ids\"]\n","            ]\n","            true_label = self.label_true(target, batch[\"tokens\"])\n","            predicted_label = self.label_pred(dec, batch[\"tokens\"])\n","            pred_mapped = self.map_tags(predicted_label)\n","            true_mapped = self.map_tags(true_label)\n","            self.true.extend(np.array(true_mapped).flatten())\n","            self.pred.extend(np.array(pred_mapped).flatten())\n","            val_loss = self._step(batch)\n","            self.log(\"val_loss\", val_loss)\n","            return {\"val_loss\": val_loss}\n","\n","        def validation_epoch_end(self, outputs):\n","            print(f\"val_epoch: {len(self.pred)}\")\n","            true_label = np.concatenate(self.true)\n","            predicted_label = np.concatenate(self.pred)\n","            cm = confusion_matrix(true_label, predicted_label)\n","            cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n","            plt.imshow(cm, cmap=\"Blues\")\n","            plt.title(\"Confusion Matrix\")\n","            plt.xlabel(\"Predicted\")\n","            plt.ylabel(\"True\")\n","            plt.colorbar()\n","            mapping = {\n","                \"O\": 0,\n","                \"rna\": 1,\n","                \"dna\": 2,\n","                \"cell_line\": 3,\n","                \"cell_type\": 4,\n","                \"protein\": 5,\n","            }\n","            reverse_mapping = {v: k for k, v in mapping.items()}\n","            ax = plt.gca()\n","            ax.set_xticks([i for i in range(len(mapping))])\n","            ax.set_yticks([i for i in range(len(mapping))])\n","            ax.set_xticklabels([reverse_mapping[i] for i in range(len(mapping))])\n","            ax.set_yticklabels([reverse_mapping[i] for i in range(len(mapping))])\n","            # wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n","            plt.clf()\n","            accuracy = accuracy_score(true_label, predicted_label)\n","            precision, recall, fscore, support = precision_recall_fscore_support(\n","                true_label, predicted_label, zero_division=1, average=\"weighted\"\n","            )\n","            # wandb.log({'precision': precision, 'recall': recall, 'f1': fscore})\n","\n","        def configure_optimizers(self):\n","            model = self.model\n","            no_decay = [\"bias\", \"LayerNorm.weight\"]\n","            optimizer_grouped_parameters = [\n","                {\n","                    \"params\": [\n","                        p\n","                        for n, p in model.named_parameters()\n","                        if not any(nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\": self.hparam.weight_decay,\n","                },\n","                {\n","                    \"params\": [\n","                        p\n","                        for n, p in model.named_parameters()\n","                        if any(nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\": 0.0,\n","                },\n","            ]\n","            optimizer = AdamW(\n","                optimizer_grouped_parameters,\n","                lr=self.hparam.learning_rate,\n","                eps=self.hparam.adam_epsilon,\n","            )\n","            self.opt = optimizer\n","            return [optimizer]\n","\n","        def optimizer_step(\n","            self,\n","            epoch=None,\n","            batch_idx=None,\n","            optimizer=None,\n","            optimizer_idx=None,\n","            optimizer_closure=None,\n","            on_tpu=None,\n","            using_native_amp=None,\n","            using_lbfgs=None,\n","        ):\n","            optimizer.step(closure=optimizer_closure)\n","            optimizer.zero_grad()\n","            self.lr_scheduler.step()\n","\n","        def get_tqdm_dict(self):\n","            tqdm_dict = {\n","                \"loss\": \"{:.3f}\".format(self.trainer.avg_loss),\n","                \"lr\": self.lr_scheduler.get_last_lr()[-1],\n","            }\n","            return tqdm_dict\n","\n","        def train_dataloader(self):\n","            train_dataset = get_dataset(\n","                tokenizer=self.tokenizer, type_path=\"train\", args=self.hparam\n","            )\n","            dataloader = DataLoader(\n","                train_dataset,\n","                batch_size=self.hparam.train_batch_size,\n","                drop_last=True,\n","                shuffle=True,\n","                num_workers=2,\n","            )\n","            t_total = (\n","                (\n","                    len(dataloader.dataset)\n","                    // (\n","                        self.hparam.train_batch_size\n","                        * max(1, self.hparam.n_gpu if torch.cuda.is_available() else 1)\n","                    )\n","                )\n","                // self.hparam.gradient_accumulation_steps\n","                * float(self.hparam.num_train_epochs)\n","            )\n","            scheduler = get_linear_schedule_with_warmup(\n","                self.opt,\n","                num_warmup_steps=self.hparam.warmup_steps,\n","                num_training_steps=t_total,\n","            )\n","            self.lr_scheduler = scheduler\n","            return dataloader\n","\n","        def val_dataloader(self):\n","            val_dataset = get_dataset(\n","                tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam\n","            )\n","            return DataLoader(\n","                val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2\n","            )\n","\n","    logger = logging.getLogger(__name__)\n","\n","    class LoggingCallback(pl.Callback):\n","        def on_validation_end(self, trainer, pl_module):\n","            logger.info(\"***** Validation results *****\")\n","            if pl_module.is_logger():\n","                metrics = trainer.callback_metrics\n","                # Log results\n","                for key in sorted(metrics):\n","                    if key not in [\"log\", \"progress_bar\"]:\n","                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","        def on_test_end(self, trainer, pl_module):\n","            logger.info(\"***** Test results *****\")\n","\n","            if pl_module.is_logger():\n","                metrics = trainer.callback_metrics\n","\n","                # Log and save results to file\n","                output_test_results_file = os.path.join(\n","                    pl_module.hparams.output_dir, \"test_results.txt\"\n","                )\n","                with open(output_test_results_file, \"w\") as writer:\n","                    for key in sorted(metrics):\n","                        if key not in [\"log\", \"progress_bar\"]:\n","                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","    args_dict = dict(\n","        data_dir=\"jnlpba\",  # path for data files\n","        output_dir=\"checkpoints\",  # path to save the checkpoints\n","        model_name_or_path=\"t5-small\",\n","        tokenizer_name_or_path=\"t5-small\",\n","        max_seq_length=512,  # todo figure out\n","        learning_rate=3e-4,\n","        weight_decay=0.0,\n","        adam_epsilon=1e-8,\n","        warmup_steps=0,\n","        train_batch_size=4,  # 4/2/1 if t5-small not working\n","        eval_batch_size=4,\n","        num_train_epochs=3,\n","        gradient_accumulation_steps=16,\n","        # n_gpu=1,\n","        early_stop_callback=False,\n","        fp_16=True,  # if you want to enable 16-bit training then install apex and set this to true\n","        opt_level=\"O1\",  # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","        max_grad_norm=1,  # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","        seed=42,\n","        val_check_interval=5,\n","    )\n","\n","    # jnlpba = load_dataset('jnlpba', split=['train[:1]', \"validation[:1]\"])\n","    # jnlpba = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n","\n","    # input_dataset_train = JnlpbDataset(tokenizer=tokenizer, dataset=jnlpba, type_path='train', portion=0)\n","\n","    # dataset_train = input_dataset_train.get_dataset()\n","\n","    # input_dataset_validation = JnlpbDataset(tokenizer=tokenizer, dataset=jnlpba, type_path='validation', portion=0)\n","    # dataset_validation = input_dataset_validation.get_dataset()\n","\n","    # datasets = DatasetDict({\"train\": dataset_train, \"validation\": dataset_validation})\n","\n","    args = argparse.Namespace(**args_dict)\n","    model = T5FineTuner(args)\n","\n","    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","        filename=args.output_dir + \"/checkpoint.pth\",\n","        monitor=\"val_loss\",\n","        mode=\"min\",\n","        save_top_k=5,\n","    )\n","\n","    train_params = dict(\n","        accumulate_grad_batches=args.gradient_accumulation_steps,\n","        # accelerator='gpu',\n","        # gpus=args.n_gpu,\n","        max_epochs=args.num_train_epochs,\n","        # early_stop_callback=False,\n","        precision=32,\n","        # amp_level=args.opt_level,\n","        gradient_clip_val=args.max_grad_norm,\n","        # checkpoint_callback=checkpoint_callback,\n","        # logger=wandb_logger,\n","        callbacks=[checkpoint_callback, LoggingCallback()],\n","    )\n","\n","    def get_dataset(tokenizer, type_path, args):\n","        tokenizer.max_length = args.max_seq_length\n","        tokenizer.model_max_length = args.max_seq_length\n","        jnlpba = load_dataset(\"jnlpba\", split=[\"train[:10]\", \"validation[:10]\"])\n","        jnlpba = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})\n","        dataset = jnlpba\n","        return JnlpbDataset(\n","            tokenizer=tokenizer, dataset=dataset, type_path=type_path, portion=0\n","        )\n","\n","    trainer = pl.Trainer(**train_params)\n","\n","    trainer.fit(model)\n","\n","    \"\"\"wandb.alert(\n","        title=\"End of training.\", \n","        text=\"Training finished successfully.\",\n","    )\n","\n","    wandb.finish()\"\"\"\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c3f7658bdd245378847585b4de4bcce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset, DatasetDict\n","\n","jnlpba = load_dataset(\"jnlpba\", split=[\"train[:10]\", \"validation[:10]\"])\n","jnlpba = DatasetDict({\"train\": jnlpba[0], \"validation\": jnlpba[1]})"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'id': '1', 'tokens': ['Number', 'of', 'glucocorticoid', 'receptors', 'in', 'lymphocytes', 'and', 'their', 'sensitivity', 'to', 'hormone', 'action', '.'], 'ner_tags': [0, 0, 9, 10, 0, 7, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"source":["print(jnlpba[\"validation\"][0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"thesis","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"f68a4539c9a11cef9bf0819cdddedfa00ec9d5fcff3291c5b30fad122c003099"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"04bb1f580b0e4430b97a4be5a8015d11":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05803d675e7c49d1a2e94e82183b8a6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1399da62475043d8a2c1b5e35bf063db","placeholder":"​","style":"IPY_MODEL_81c5cbfcca2f495282a2ef0a9e59aa3b","value":" 2/2 [00:00&lt;00:00, 50.86it/s]"}},"07050080cdbe4feca8bd9f54fca26537":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0aa8f40b4d974319ae78a03fcdd7ef05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c4a2dfb4db34a6987218d14fca65671":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e37623fb7cbf4de5bfc4aede2aea8e68","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07050080cdbe4feca8bd9f54fca26537","value":2}},"0ee1dc4cc8484ee7b1586a2dd6fabd64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10a02524699d4a85966d7347eceb242f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9f00ac026fd4a4588f3aebf056607b3","placeholder":"​","style":"IPY_MODEL_2f91866e03914919ae527782c6cd1b09","value":"100%"}},"10bb57b11c22494e96ff8932f6faf929":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1399da62475043d8a2c1b5e35bf063db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13dfe00586ca4db8b105659825c9695d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_659b7b033b6f42a68d0c6b9599f51428","placeholder":"​","style":"IPY_MODEL_ad456d61621d49e09b61c74b3fbb3f1a","value":"Downloading: 100%"}},"191793bf1c954a4094e658923b8d7ea0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cc7e51154004f1c8ab939f2d519aa86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d347a76c2f648c2897c24bbd6e270e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21be4a80c2824249a26a73c00a063427":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ee1dc4cc8484ee7b1586a2dd6fabd64","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_537faac266c74412a45fc025847d1d5e","value":2}},"2f39b3b8167b408e979f99885f0a73be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c06b9f7f34e94b87bc7cc5d240b11623","placeholder":"​","style":"IPY_MODEL_aabb344daf234eee9ca19075a3b56f5b","value":" 2/2 [00:00&lt;00:00, 95.83it/s]"}},"2f745a3b464d43d9970496fbaa96a9d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f91866e03914919ae527782c6cd1b09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"308ce58d8c424310842f9d3e825a84b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13dfe00586ca4db8b105659825c9695d","IPY_MODEL_93fc274c1ea34a0ebf03e9965de84fd0","IPY_MODEL_6acf4cad08ae4c2290e99336e04f8906"],"layout":"IPY_MODEL_af21012d0e9341da9a559e39c49a9beb"}},"318fcef6589c4d1b866f66d774a86a43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"3dda6b69c20a43fa8166c6aef077b5d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4a9cd938011412fb7a36d5850f60056","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e30f6ce4fef949efaa11b06a6edc1a63","value":2}},"3ddf0d93e284407c9d5487aba1075a1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b809477682c49a8bc7ecfb8edef0a37","placeholder":"​","style":"IPY_MODEL_fd2b69e1f33c457ea789eb9ae5a95951","value":"Epoch 1: 100%"}},"446da3c205564ab5a44bb7e9d99d4737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44fc0cc48ec94431a7908b3af5e6ad9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52dc301529d647cd99743e227e964457":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"537faac266c74412a45fc025847d1d5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"557bbbf65f0447edae3f4275d9fdfcfd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56cd73ac22f9482c98c47ecfbc79fd22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5bde5f887a4b4453b1c7f063571eebf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ddf0d93e284407c9d5487aba1075a1b","IPY_MODEL_b39d9d4171aa4ac3901f3f565dbdedb9","IPY_MODEL_ed58aa2dbf014ba397dd3dfc92748f4f"],"layout":"IPY_MODEL_d81b53cabf644fb888003bb5226b4b69"}},"5f530bc9d6474de3919d0e157a7a043d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10bb57b11c22494e96ff8932f6faf929","placeholder":"​","style":"IPY_MODEL_c3ba56b10be24daf8601d5f765b11531","value":"Validation DataLoader 0: 100%"}},"618993de1f5d4306aba1ea17a55ca811":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6412440adaf24a1f89385f50f3c27cbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"659b7b033b6f42a68d0c6b9599f51428":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"687e9859f5054ac9965ce1fdfe10335f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6acf4cad08ae4c2290e99336e04f8906":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c328272ecf8c476eb6beb8af43c7c88e","placeholder":"​","style":"IPY_MODEL_afbbe85f472b4e65bfbfa4c3e738ca4e","value":" 242M/242M [00:03&lt;00:00, 72.3MB/s]"}},"6aeb25c4ca724ceb837ff3d3d9e34bbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e96ccdf826914c908e4bd2c2d65097fc","IPY_MODEL_f8382a174e324beaac03ccb096ed42b0","IPY_MODEL_a9d9060d50874a58b9fb3c947a01e552"],"layout":"IPY_MODEL_318fcef6589c4d1b866f66d774a86a43"}},"747f8ef28c894212b638e0f2228692e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10a02524699d4a85966d7347eceb242f","IPY_MODEL_cd65ae9aa2ce42beb252abf37a224fb6","IPY_MODEL_05803d675e7c49d1a2e94e82183b8a6c"],"layout":"IPY_MODEL_618993de1f5d4306aba1ea17a55ca811"}},"7c8a31fb9dea4c709a25fcaf2bd7ccf2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f2f18313a9b48a281b742f765eee68a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e108d8cfdce84a9386c60a57c0ccdc91","placeholder":"​","style":"IPY_MODEL_446da3c205564ab5a44bb7e9d99d4737","value":" 2/2 [00:00&lt;00:00, 25.02it/s]"}},"81c5cbfcca2f495282a2ef0a9e59aa3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b3951c85bd74b668c3094c31029c72a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7061b0bbaf6458ca9518f3a67cb6fd1","placeholder":"​","style":"IPY_MODEL_1d347a76c2f648c2897c24bbd6e270e1","value":" 2/2 [00:02&lt;00:00,  1.46s/it]"}},"8b809477682c49a8bc7ecfb8edef0a37":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d3ba9e01f5747d7a8f8ca32c65853eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905065f70d2846c0b3ffd43edb9d209b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44fc0cc48ec94431a7908b3af5e6ad9b","placeholder":"​","style":"IPY_MODEL_e14ebba3c7024183aac18b2d61c280c1","value":"100%"}},"93dfb127f3684234a009164edacb7b89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfa2aa1fffb7413096f5716dbc3fae9a","IPY_MODEL_21be4a80c2824249a26a73c00a063427","IPY_MODEL_9802821e7b2341cca9f3be4283688120"],"layout":"IPY_MODEL_191793bf1c954a4094e658923b8d7ea0"}},"93fc274c1ea34a0ebf03e9965de84fd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6dfcc20949d434cb38d2caa56acf958","max":242065649,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe444eac38af4dd5b8854005db712bc7","value":242065649}},"9802821e7b2341cca9f3be4283688120":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04bb1f580b0e4430b97a4be5a8015d11","placeholder":"​","style":"IPY_MODEL_fa3d9216d1a84191b7338946619a9769","value":" 2/2 [00:00&lt;00:00, 76.88it/s]"}},"9e05266880874bc297ea8daff5b6d064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6dfcc20949d434cb38d2caa56acf958":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a948414581d94776b6d4e4dc5ad95023":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_905065f70d2846c0b3ffd43edb9d209b","IPY_MODEL_0c4a2dfb4db34a6987218d14fca65671","IPY_MODEL_2f39b3b8167b408e979f99885f0a73be"],"layout":"IPY_MODEL_cc15cd85d45b4fd89db6397133faeee8"}},"a9d9060d50874a58b9fb3c947a01e552":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d3ba9e01f5747d7a8f8ca32c65853eb","placeholder":"​","style":"IPY_MODEL_e40b957f198b48099a55d684cd6c4df5","value":" 2/2 [00:00&lt;00:00, 24.26it/s]"}},"aabb344daf234eee9ca19075a3b56f5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aae1252dfd084067987adb777127fa83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abea09c7219b48e691287df0a2a8a689":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ac5eb07253b746aba1d5e084fffc61ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ad456d61621d49e09b61c74b3fbb3f1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae6a42abd7bc4251836976922330d02e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af21012d0e9341da9a559e39c49a9beb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afbbe85f472b4e65bfbfa4c3e738ca4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0f4ec4e56ce461dad9eae3e8bfa8542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f530bc9d6474de3919d0e157a7a043d","IPY_MODEL_f417fe0f56b1419eb393d4b9bb7cac4f","IPY_MODEL_7f2f18313a9b48a281b742f765eee68a"],"layout":"IPY_MODEL_abea09c7219b48e691287df0a2a8a689"}},"b39d9d4171aa4ac3901f3f565dbdedb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6412440adaf24a1f89385f50f3c27cbe","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae6a42abd7bc4251836976922330d02e","value":3}},"bcc6009bc67b4a5f9b24f67652690af5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfa2aa1fffb7413096f5716dbc3fae9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c8a31fb9dea4c709a25fcaf2bd7ccf2","placeholder":"​","style":"IPY_MODEL_bcc6009bc67b4a5f9b24f67652690af5","value":"100%"}},"c06b9f7f34e94b87bc7cc5d240b11623":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1bd319c1d394a3595a34a7c17adea07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f75e0f5fcad947baa6353bf2b4fe81cb","IPY_MODEL_3dda6b69c20a43fa8166c6aef077b5d5","IPY_MODEL_8b3951c85bd74b668c3094c31029c72a"],"layout":"IPY_MODEL_ac5eb07253b746aba1d5e084fffc61ac"}},"c328272ecf8c476eb6beb8af43c7c88e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3ba56b10be24daf8601d5f765b11531":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9f00ac026fd4a4588f3aebf056607b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc15cd85d45b4fd89db6397133faeee8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd65ae9aa2ce42beb252abf37a224fb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cea5530aad974e379098d320bde565f8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56cd73ac22f9482c98c47ecfbc79fd22","value":2}},"cea5530aad974e379098d320bde565f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2795d6eb6db498e843f36064a151496":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4a9cd938011412fb7a36d5850f60056":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7061b0bbaf6458ca9518f3a67cb6fd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d81b53cabf644fb888003bb5226b4b69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e108d8cfdce84a9386c60a57c0ccdc91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e14ebba3c7024183aac18b2d61c280c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e30f6ce4fef949efaa11b06a6edc1a63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e37623fb7cbf4de5bfc4aede2aea8e68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e40b957f198b48099a55d684cd6c4df5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e44553f939c2420f989debe0d85890c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e96ccdf826914c908e4bd2c2d65097fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_687e9859f5054ac9965ce1fdfe10335f","placeholder":"​","style":"IPY_MODEL_1cc7e51154004f1c8ab939f2d519aa86","value":"Validation DataLoader 0: 100%"}},"ed58aa2dbf014ba397dd3dfc92748f4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e44553f939c2420f989debe0d85890c6","placeholder":"​","style":"IPY_MODEL_d2795d6eb6db498e843f36064a151496","value":" 3/3 [00:01&lt;00:00,  2.30it/s, loss=2.58, v_num=0, val_loss=3.170]"}},"f417fe0f56b1419eb393d4b9bb7cac4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aa8f40b4d974319ae78a03fcdd7ef05","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e05266880874bc297ea8daff5b6d064","value":2}},"f75e0f5fcad947baa6353bf2b4fe81cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_557bbbf65f0447edae3f4275d9fdfcfd","placeholder":"​","style":"IPY_MODEL_52dc301529d647cd99743e227e964457","value":"Sanity Checking DataLoader 0: 100%"}},"f8382a174e324beaac03ccb096ed42b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_aae1252dfd084067987adb777127fa83","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f745a3b464d43d9970496fbaa96a9d5","value":2}},"fa3d9216d1a84191b7338946619a9769":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd2b69e1f33c457ea789eb9ae5a95951":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe444eac38af4dd5b8854005db712bc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
