{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text infilling: context, T, attribute, A, and the value, V , in a sentence as “T. A is V .” where the attribute value V is masked as blank\n",
    "\n",
    "Answer generation: generate V as the answer, considering T as the context and A as the question.\n",
    "\n",
    "Answer generation seems to be plausible. I just train the a model on that. I will just use T5 because its not important for me to have the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "from transformers import(\n",
    "    AdamW,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3c7f748eda44ed9da67fb5acbee573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'tokens': ['IL-2', 'gene', 'expression', 'and', 'NF-kappa', 'B', 'activation', 'through', 'CD28', 'requires', 'reactive', 'oxygen', 'production', 'by', '5-lipoxygenase', '.'], 'ner_tags': [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "jnlpba = load_dataset('jnlpba', split=['train[:10]', \"validation[:10]\"])\n",
    "print(jnlpba[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a df with question, context and answer like:\n",
    "\n",
    "What are the entities?\n",
    "\n",
    "IL-2', 'gene', 'expression', 'and', 'NF-kappa', 'B', 'activation', 'through', 'CD28', 'requires', 'reactive', 'oxygen', 'production','by', '5-lipoxygenase', '.\n",
    "\n",
    "attr1, attr2, attr3\n",
    "\n",
    "The question I still have is how i need to tokenize everything - how i am going to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                             tokens  \\\n",
      "0  1  [IL-2, gene, expression, and, NF-kappa, B, act...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [1, 2, 0, 0, 9, 10, 0, 0, 9, 0, 0, 0, 0, 0, 9, 0]  \n",
      "  id                                             tokens  \\\n",
      "0  1  IL-2 gene expression and NF-kappa B activation...   \n",
      "1  2  Activation of the CD28 surface receptor provid...   \n",
      "2  3  In primary T lymphocytes we show that CD28 lig...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  B-DNA I-DNA O O B-protein I-protein O O B-prot...  \n",
      "1  O O O B-protein I-protein I-protein O O O O O ...  \n",
      "2  O B-cell_type I-cell_type I-cell_type O O O B-...  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(jnlpba[0])\n",
    "print(df_train.head(1))\n",
    "df_val = pd.DataFrame(jnlpba[1])\n",
    "\n",
    "mapping = {\n",
    "    0: \"O\",\n",
    "    1: \"B-DNA\",\n",
    "    2: \"I-DNA\",\n",
    "    3: \"B-RNA\",\n",
    "    4: \"I-RNA\",\n",
    "    5: \"B-cell_line\",\n",
    "    6: \"I-cell_line\",\n",
    "    7: \"B-cell_type\",\n",
    "    8: \"I-cell_type\",\n",
    "    9: \"B-protein\",\n",
    "    10: \"I-protein\"\n",
    "}\n",
    "\n",
    "def map_tags(row):\n",
    "    row['ner_tags'] = [' '.join([mapping[tag] for tag in row['ner_tags']])][0]\n",
    "    return row\n",
    "\n",
    "df_train = df_train.apply(map_tags, axis=1)\n",
    "df_val = df_val.apply(map_tags, axis=1)\n",
    "\n",
    "def join_tags(row):\n",
    "    row['tokens'] = ' '.join(row['tokens'])\n",
    "    return row\n",
    "\n",
    "df_train = df_train.apply(join_tags, axis=1)\n",
    "df_val = df_val.apply(join_tags, axis=1)\n",
    "\n",
    "print(df_train.head(3))\n",
    "\n",
    "data_train = DatasetDict({\"context\": df_train['tokens'], \"question\": \"What are the attributes?\", \"answer\": df_train['ner_tags']})\n",
    "data_val = DatasetDict({\"context\": df_val['tokens'], \"question\": \"What are the attributes?\", \"answer\": df_val['ner_tags']})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize input (context)\n",
    "input_tensors = [tokenizer.encode(sentence, return_tensors='pt') for sentence in data_train['context']]\n",
    "\n",
    "# tokenize output (answer)\n",
    "output_tensors = [tokenizer.encode(sentence, return_tensors='pt') for sentence in data_train['answer']]\n",
    "\n",
    "# create traning arguments\n",
    "#todo check if 16 is appropriate\n",
    "batch_size = 16\n",
    "args = TrainingArguments(\n",
    "    \"T5-fine-tuned-NER-QA\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #report_to=\"wandb\",\n",
    "    #push_to_hub=True,\n",
    "    #todo hide token \n",
    "    #push_to_hub_token=\"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\"\n",
    ")\n",
    "\n",
    "# train \n",
    "model = transformers.T5Model.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # created based on collator which is used in this example https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb#scrollTo=TlqNaB8jIrJW\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Convert the input data to tensors and build the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_tensors \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mencode(sentence, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m data_train[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m----> 3\u001b[0m output_tensors \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mtensor(tags, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong) \u001b[39mfor\u001b[39;00m tags \u001b[39min\u001b[39;00m data_train[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(input_tensors, output_tensors)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Use the BatchTransform class to create batches of input and output tensors\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [22], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Convert the input data to tensors and build the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_tensors \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mencode(sentence, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m data_train[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m----> 3\u001b[0m output_tensors \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mtensor(tags, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong) \u001b[39mfor\u001b[39;00m tags \u001b[39min\u001b[39;00m data_train[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(input_tensors, output_tensors)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Use the BatchTransform class to create batches of input and output tensors\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Convert the input data to tensors and build the dataset\n",
    "input_tensors = [tokenizer.encode(sentence, return_tensors='pt') for sentence in data_train['context']]\n",
    "output_tensors = [torch.tensor(tags, dtype=torch.long) for tags in data_train['answer']]\n",
    "dataset = torch.utils.data.TensorDataset(input_tensors, output_tensors)\n",
    "\n",
    "# Use the BatchTransform class to create batches of input and output tensors\n",
    "data_transform = transformers.BatchTransform(batch_size=batch_size)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=data_transform.collate_fn)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the number of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    3,  3502,  4949,  6510,  3893,    11,     3, 12619,    18,   157,\n",
      "          3096,     9,   272,  5817,   257,   190,  3190,  2577,  2311, 28360,\n",
      "         11035,   999,    57,  7670,  7446,  9773,   729,     9,     7,    15,\n",
      "             3,     5,     1]])\n"
     ]
    }
   ],
   "source": [
    "#print(data_train)\n",
    "context_max_token_len = max(data_train['context'], key=len)\n",
    "encoding_context = tokenizer.encode(\n",
    "    data_train['context'][0],\n",
    "    return_tensors='pt'\n",
    ")\n",
    "#i need somehow convert \n",
    "#i can just try to use the tokenizer on datatrain\n",
    "#now I tokenize this shit but what is next?\n",
    "#do i pu\n",
    "print(encoding_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i need to tokenize the data. now i am doing this with the ner dataset class. instead i could do it manually. \n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: T5Tokenizer, answer_max_token_len: int = 0 , question_max_token_len: int=0, context_max_token_len: int=0):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.answer_max_token_len = max(data['context'], key=len)\n",
    "        self.question_max_token_len = max(data['question'], key=len)\n",
    "        self.context_max_token_len = max(data['context'], key=len)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        encoding_context = tokenizer(    \n",
    "            data_row[\"context\"],\n",
    "            max_length=int(self.context_max_token_len),\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        encoding_answer = tokenizer(\n",
    "            data_row[\"answer\"],\n",
    "            max_length=int(self.answer_max_token_len),\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"   \n",
    "        )\n",
    "        \n",
    "        encoding_question = tokenizer(\n",
    "            data_row[\"question\"],\n",
    "            max_length=int(self.question_max_token_len),\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = encoding_answer['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            question = data_row[\"question\"],\n",
    "            context = data_row[\"context\"],\n",
    "            answer = data_row[\"answer\"],\n",
    "            input_ids = encoding_context['input_ids'].flatten(),\n",
    "            attention_mask = encoding_context['attention_mask'].flatten(),\n",
    "            labels = labels.flatten()   \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset_train = NERDataset(data_train, tokenizer)\n",
    "sample_dataset_val = NERDataset(data_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'In primary T lymphocytes we show that CD28 ligation leads to the rapid intracellular formation of reactive oxygen intermediates ( ROIs ) which are required for CD28 -mediated activation of the NF-kap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m sample_dataset_train:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [9], line 19\u001b[0m, in \u001b[0;36mNERDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m     15\u001b[0m     data_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39miloc[index]\n\u001b[1;32m     17\u001b[0m     encoding_context \u001b[39m=\u001b[39m tokenizer(    \n\u001b[1;32m     18\u001b[0m         data_row[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m---> 19\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext_max_token_len),\n\u001b[1;32m     20\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monly_second\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         return_attention_mask\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m         add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     encoding_answer \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     28\u001b[0m         data_row[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     29\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswer_max_token_len),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m   \n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     encoding_question \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     38\u001b[0m         data_row[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m         max_length\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquestion_max_token_len),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'In primary T lymphocytes we show that CD28 ligation leads to the rapid intracellular formation of reactive oxygen intermediates ( ROIs ) which are required for CD28 -mediated activation of the NF-kap"
     ]
    }
   ],
   "source": [
    "for i in sample_dataset_train:\n",
    "    print(i)\n",
    "    break\n",
    "#next i need to train my model - i can do this direct with huggingface instead with pytorch lightning \n",
    "#first i need to check why my dataset is not converted to tensors\n",
    "#i just want to tokenize my data and thats it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still need to turn the attributes representation into a string and not list done\n",
    "#what is next. nex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 10:44:50) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f68a4539c9a11cef9bf0819cdddedfa00ec9d5fcff3291c5b30fad122c003099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
