{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes \n",
    "\n",
    "- [x] check uni requirements - with fontsize always talk with gutachter about it (like in my expose is okay)\n",
    "- [x] check if I can convert notebook to markdown and markdown than to pdf like i did in expose \n",
    "- [x] create header grundlagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 1. Abstract\n",
    "# 2. Einführung\n",
    "    2.1 Motivation (bspw. Anwendungen von NER),\n",
    "    2.2 Problem\n",
    "    2.3 Ziele (bspw. Überblick über bestehende Ansätze, Analyse und Vergleich der drei gängigsten Ansätze),\n",
    "    2.4 Struktur der Arbeit\n",
    "# 3. Grundlagen\n",
    "    3.1 Neuronale Netzwerke\n",
    "    3.2 Deep Learning \n",
    "    3.3 Backpropagation\n",
    "    3.3 NLP (Erklärung des gesamten Prozess von Wort zu Vektor und die verschiedenen Bereiche, wo diese Vektoren eingesetzt werden können (Textgenerierung, Textzusammenfassungen...))\n",
    "    3.4 NER \n",
    "    3.5 Labelling \n",
    "    3.6 Seq2Seq\n",
    "    3.7 LSTM/RNN \n",
    "    3.8 Transformer\n",
    "    3.9 Fine tuning \n",
    "# 4. Tools\n",
    "    4.2 PyTorch\n",
    "    4.1 Huggingface \n",
    "    ...\n",
    "    ...\n",
    "# 5. Methode \n",
    "    5.1 Training mit schwach gelabelten Datensatz \n",
    "    5.2 Training mit selbst gelabelten Datensatz \n",
    "    5.3 Generatives Modell\n",
    "# 6. Design der Experimente (inkl. Modell/e, Daten)\n",
    "    6.1 Training mit schwach gelabelten Datensatz \n",
    "    6.2 Training mit selbst gelabelten Datensatz \n",
    "    6.3 Generatives Modell\n",
    "# 7. Ergebnisse der Experimente\n",
    "    7.1 Training mit schwach gelabelten Datensatz \n",
    "    7.2 Training mit selbst gelabelten Datensatz \n",
    "    7.3 Generatives Modell\n",
    "# 8. Diskussion (inkl. wichtiger Ergebnisse, Einschränkungen und zukünftiger Arbeiten, Implikationen für Forschung und Praxis)\n",
    "# 9. Zusammenfassung\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) ist ein weitverbreiteter Ansatz für die Analyse von Textblöcken und das Auffinden und Klassifizieren vordefinierter Eigennamen wie Standort, Unternehmen oder Personennamen. Da die Arbeit im zusammenarbeit mit ML6 angefertigt wurde - ein Unternehmen, welches sich darauf spezialisiert hat spezifische Lösungen für machine learning Probleme zu entwickeln - konnte der Anwendungsfall von einen der im Unternehmen ablaufenden Projekte abgeleitet werden. Das Unternehmen hat die Aufgabe Such... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlagen "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Arbeit basiert vor allem auf Transformer Modelle. Für das erstellen von Transformer Modelle bedarf es wissen verschiedener Grundlagen. Folgend werden Grundlagen Technologien beschrieben. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronale Netze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurale Netzwerke sind Algorithmen die dafür entwickelt wurden die Funktionen eines Gehirns nachzubilden. \n",
    "Der erste Algorithmus der dazu entwickelt wurde ist das McCulloch-Pitts Neuronen Model. Dabei handelt sich um folgende Funktion:\n",
    "\n",
    "$$y = 1 \\text{ wenn } w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_n \\cdot x_n \\ge \\theta$$\n",
    "$$y = 0 \\text{ sonst }$$\n",
    "\n",
    "$y=$ Ausgabe des Neurons\n",
    "\n",
    "$x_n=$ Eingabe des Neurons\n",
    "\n",
    "$w_n=$ Gewichte des Neurons\n",
    "\n",
    "$\\theta=$ Schwellenwert\n",
    "\n",
    "Die Funktion lässt sich bildlich darstellen:\n",
    "\n",
    "![MP-Neuron](images/MP_Neuron.drawio.svg) [todo add weights to image]\n",
    "\n",
    "Der nächste Schritt in der Historie neuronaler Netzwerke war die Entwicklung des Perceptrons [todo wer und wann]. Der Aufbau ist ähnlich zu dem des MP Neuronen Model unterscheidet sich aber darin, dass der Schellenwert kontinuierlich ist. Das heißt ein nach dem Input von Eingaben Werten kann der Output zum Beispiel 0.7 annehmen wohin bei einem linearen Schwellenwert das Ergebnis entweder 1 oder 0 sein kann. Anstatt nur die Schwellenwert Funktion kann das Perzeptron auch andere Aktivierungs funktionen annehmen [figure].\n",
    "\n",
    "Folgend eine Tabelle, welche die Unterschiede aufzeigt:\n",
    "\n",
    "| Beschreibung | MP Neuron | Perzeptron |\n",
    "|---------|---------------------|----------------------|\n",
    "| Typ des Modells | Binär | Linear |\n",
    "| Schwellenwert | Statisch | Anpassbar während des Trainings |\n",
    "| Ausgabe | Binär (1 oder 0) | Kontinuierliche Werte |\n",
    "\n",
    "![MP-Neuron](images/Perceptron.drawio.svg) [todo add weights to image]\n",
    "\n",
    "\n",
    "[todo multi layer perzeptron]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation ist eine Methode welche in künstlichen Neuronalen netzwerken benutzt wird um die Gewichte von einem neuronalen Netzwerk anzupassen um vorhersagen basierend auf einem Datenset liefern zu können. Der Algorithmus betrachtet zuerst die Gewichte an der Ausgabeschicht und geht von dort aus bis hin zur Eingabeschicht. \n",
    "\n",
    "Angenommen man hat zwei Neuronen $x_1$ und $x_2$ in der Eingabeschicht, zwei Neuronen $h_1$ und $h_2$ in der versteckten Schicht, $y$ als Ausgangsneuron und die Gewichte $w1, w2, w3, w4, w5 \\text{ und } w6$ zwischen den Neuronen.\n",
    "\n",
    "Dabei berechnet man die Ausgabe $\\hat{y}$ folgendermaßen:\n",
    "\n",
    "$\\hat{y} = f(w1 * x1 + w2 * x2 + w3 * h1 + w4 * h2)$\n",
    "\n",
    "$f$ ist die Aktivierungsfunktion.\n",
    "\n",
    "Um die Ausgabe dahingehend zu verändern genauere Ergebnisse für eine Eingabedaten zu erhalten verwendet man Backpropagation um die Werte der einzelnen Gewichte so anzupassen genauere Ergebnisse zu erhalten. Bei einem Durchlauf kommt es zu dem Ergebnis $\\hat{y}$. Das ist die tatsächliche Ausgabe eines neuronalen Netzwerkes nach einem Durchlauf der Daten. Die tatsächliche Ausgabe wird mit dem Wert der erwarteten Ausgabe verglichen. Man berechnet den Fehler $\\delta$ dieser zwei Werte. \n",
    "\n",
    "$\\delta = (y - \\hat{y})^2$ \n",
    "\n",
    "Für den Fehler ist es möglich verschiedene mathematische Funktionen zu benutzen. Eine übliche Funktion ist die der mittleren quadratischen Abweichung.\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$\n",
    "\n",
    "Die Funktion quadriert jedes einzelne Fehlersignal und summt am Ende alles zusammen und teilt es durch die Anzahl der Datenpunkte. Das Ergebnis ist der durchschnittliche Fehler aller quadrierten Datenpunkte. Es wird quadriert, weil die Ergebnisse sich dadurch leichter weiterverarbeiten lassen.\n",
    "\n",
    "Wenn man den Fehler an der Ausgabe berechnet hat kann man den Durchlauf wieder zurück gehen und als erstes den Fehler an den Neuronen in der versteckten Schicht berechnen. \n",
    "\n",
    "Backpropagation, or backprop, is a supervised learning algorithm commonly used in training feedforward neural networks. The goal of backprop is to compute the gradient of the loss function with respect to the weights of the network, so that the weights can be updated in the opposite direction of the gradient to reduce the loss.\n",
    "\n",
    "Backprop works by starting with the final layer of the network, called the output layer, and computing the error of the output with respect to the true or desired output using a loss function, such as the mean squared error. This error is then propagated backwards through the network, layer by layer, using the chain rule to compute the gradient of the loss with respect to the weights at each layer.\n",
    "\n",
    "Here is a more detailed explanation of the steps involved in backpropagation:\n",
    "\n",
    "Forward propagation: The input data is fed into the input layer of the network, and then it is passed through the network layer by layer, with the output of each layer being used as the input for the next layer. At each layer, the input is multiplied by the weights of the layer, and then it is passed through a non-linear activation function, such as the sigmoid function, to produce the output of the layer. This process continues until the final layer, the output layer, produces a predicted output for the input data.\n",
    "\n",
    "Loss calculation: The error of the predicted output is then calculated using the loss function, such as the mean squared error. This measures how well the model's predicted output matches the true or desired output.\n",
    "\n",
    "Backward propagation: The error at the output layer is then propagated backwards through the network, layer by layer, using the chain rule to compute the gradient of the loss with respect to the weights at each layer. This is done by first computing the gradient of the loss with respect to the output of each node in the output layer, and then using this gradient to compute the gradient of the loss with respect to the inputs of each node in the output layer. This process continues backwards through the network, until the gradient of the loss with respect to the inputs of the input layer is computed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Als nächstes berechnet man den Fehler an jedem Neuron im Netzwerk.\n",
    "\n",
    "Next, we calculate the error at each node in the network. To do this, we multiply the error at the output node by the derivative of the activation function at the output node. This gives us the error at the output node.\n",
    "\n",
    "Then, we propagate the error back through the network, starting at the output node and working our way back to the input nodes. To do this, we multiply the error at each node by the weights of the connections between that node and the nodes it is connected to. This gives us the error at each node in the network.\n",
    "\n",
    "Finally, we use this error to adjust the weights of the connections between the nodes. This is done using a learning rate, which determines the size of the adjustments we make to the weights.\n",
    "\n",
    "1. \n",
    "2. Von den Ausgabewerten wird die Fehlerrate berechnet und mit der tatsächlichen Fehlerrate verglichen. \n",
    "3. \n",
    "\n",
    "Um den Algorithmus ausführen zu können müssen verschiedene Bedingungen erfüllt sein. Es muss mindestens \n",
    "\n",
    "backpropagation is a method used in artificial neural networks to adjust the weights of the network in order to improve its performance. It involves starting with the output layer and working backwards through the hidden layers to the input layer, computing the gradient of the error function with respect to each weight. This allows the network to learn to map input data to output data, and to improve its predictions over time. Backpropagation is a key method used in the training of neural networks, and is widely used in machine learning and artificial intelligence applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning ist die weiterentwicklung der vorherig beschriebenen einfachen neuralen netzwerken und ein Teilbereich des  maschinellen lernens. \n",
    "\n",
    "[todo bild mit einordnung verschiedener Bereiche]\n",
    "\n",
    "Anstatt von einer Schicht, welche die Eingabewerte von Neuronen verarbeitet gibt es bei einem Deep Learning Modell mehrere Schichten. Die genaue Anzahl an Layern, die benötigt werden um ein Modell ein Deep Learning werden zu lassen ist nicht definiert. Der Begriff Deep Learning ist also ein eher allgemeiner Begriff der Modelle beschreibt die mehrere Schichten enthalten. Eine Schicht besteht jeweils aus Eingabewerten, einer Funktion welche die Eingabewerte verarbeitet und Ausgabewerte erzeugt. \n",
    "\n",
    "[todo bild mit deep learning model mit mehreren Schichten]\n",
    "\n",
    "Es gibt verschiedene Architekturen, die für verschiedene Arten des lernens von Daten eingsetzt werden. Beispiele für Architekturen neuronaler Netzwerke sind auf folgender Grafik gut einsehbar.\n",
    "\n",
    "[link to image from https://www.asimovinstitute.org/neural-network-zoo/]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processsing (NLP) ist ein Teilbereich der Linguistik und des maschinellen lernens. Das verarbeiten von menschlicher Sprache findet eine breite Anwendung in verschiedensten Bereichen. Beispiele sind Entitäten Erkennung, Maschinenübersetzung, Spracherkennung, Sentiment Analyse. Dadurch, die sinnvolle weiterverarbeitung von Sprache eine komplexe Aufgabe ist, dadurch Wörter in verschiedene Kontexten zum Beispiel unterschiedlich Deutung besitzen können muss ein Deep Learning Modell mit viel Daten trainiert werden um den verschiedenen Kontext zu erkennen.\n",
    "\n",
    "Beispiel für unterschiedliche Wortbedeutung in unterschiedlichen Zusammenhängen:\n",
    "\n",
    "Neben der Kirche befindet sich eine Bank.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bei NLP kommt es oft zu der Notwendigkeit mit großen Mengen an Daten zu arbeiten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP algorithms require a lot of data in order to learn the complex and varied rules of natural language. This is because natural language is highly contextual and can be ambiguous, so the algorithms need to be exposed to a large number of examples in order to accurately identify the correct meaning of words and sentences. Additionally, the algorithms used in NLP are typically machine learning models, which are a type of algorithm that can improve their performance on a specific task by being trained on large amounts of data. Therefore, the more data these algorithms are trained on, the more accurate they are likely to be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
