{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes \n",
    "\n",
    "- [x] check uni requirements - with fontsize always talk with gutachter about it (like in my expose is okay)\n",
    "- [x] check if I can convert notebook to markdown and markdown than to pdf like i did in expose \n",
    "- [x] create header grundlagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 1. Abstract\n",
    "# 2. Einführung\n",
    "    2.1 Motivation (bspw. Anwendungen von NER),\n",
    "    2.2 Problem\n",
    "    2.3 Ziele (bspw. Überblick über bestehende Ansätze, Analyse und Vergleich der drei gängigsten Ansätze),\n",
    "    2.4 Struktur der Arbeit\n",
    "# 3. Grundlagen\n",
    "    3.1 Neuronale Netzwerke\n",
    "    3.2 Deep Learning \n",
    "    3.3 Backpropagation\n",
    "    3.3 NLP (Erklärung des gesamten Prozess von Wort zu Vektor und die verschiedenen Bereiche, wo diese Vektoren eingesetzt werden können (Textgenerierung, Textzusammenfassungen...))\n",
    "    3.4 NER \n",
    "    3.5 Labelling \n",
    "    3.6 Seq2Seq\n",
    "    3.7 LSTM/RNN \n",
    "    3.8 Transformer\n",
    "    3.9 Fine tuning \n",
    "# 4. Tools\n",
    "    4.2 PyTorch\n",
    "    4.1 Huggingface \n",
    "    ...\n",
    "    ...\n",
    "# 5. Methode \n",
    "    5.1 Training mit schwach gelabelten Datensatz \n",
    "    5.2 Training mit selbst gelabelten Datensatz \n",
    "    5.3 Generatives Modell\n",
    "# 6. Design der Experimente (inkl. Modell/e, Daten)\n",
    "    6.1 Training mit schwach gelabelten Datensatz \n",
    "    6.2 Training mit selbst gelabelten Datensatz \n",
    "    6.3 Generatives Modell\n",
    "# 7. Ergebnisse der Experimente\n",
    "    7.1 Training mit schwach gelabelten Datensatz \n",
    "    7.2 Training mit selbst gelabelten Datensatz \n",
    "    7.3 Generatives Modell\n",
    "# 8. Diskussion (inkl. wichtiger Ergebnisse, Einschränkungen und zukünftiger Arbeiten, Implikationen für Forschung und Praxis)\n",
    "# 9. Zusammenfassung\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) ist ein weitverbreiteter Ansatz für die Analyse von Textblöcken und das Auffinden und Klassifizieren vordefinierter Eigennamen wie Standort, Unternehmen oder Personennamen. Da die Arbeit im zusammenarbeit mit ML6 angefertigt wurde - ein Unternehmen, welches sich darauf spezialisiert hat spezifische Lösungen für machine learning Probleme zu entwickeln - konnte der Anwendungsfall von einen der im Unternehmen ablaufenden Projekte abgeleitet werden. Das Unternehmen hat die Aufgabe Such... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlagen "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Arbeit basiert vor allem auf Transformer Modelle. Für das erstellen von Transformer Modelle bedarf es wissen verschiedener Grundlagen. Folgend werden Grundlagen Technologien beschrieben. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuronale Netze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurale Netzwerke sind Algorithmen die dafür entwickelt wurden die Funktionen eines Gehirns nachzubilden. \n",
    "Der erste Algorithmus der dazu entwickelt wurde ist das McCulloch-Pitts Neuronen Model. Dabei handelt sich um folgende Funktion:\n",
    "\n",
    "$$y = 1 \\text{ wenn } w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_n \\cdot x_n \\ge \\theta$$\n",
    "$$y = 0 \\text{ sonst }$$\n",
    "\n",
    "$y=$ Ausgabe des Neurons\n",
    "\n",
    "$x_n=$ Eingabe des Neurons\n",
    "\n",
    "$w_n=$ Gewichte des Neurons\n",
    "\n",
    "$\\theta=$ Schwellenwert\n",
    "\n",
    "Die Funktion lässt sich bildlich darstellen:\n",
    "\n",
    "![MP-Neuron](images/MP_Neuron.drawio.svg) [todo add weights to image]\n",
    "\n",
    "Der nächste Schritt in der Historie neuronaler Netzwerke war die Entwicklung des Perceptrons [todo wer und wann]. Der Aufbau ist ähnlich zu dem des MP Neuronen Model unterscheidet sich aber darin, dass der Schellenwert kontinuierlich ist. Das heißt ein nach dem Input von Eingaben Werten kann der Output zum Beispiel 0.7 annehmen wohin bei einem linearen Schwellenwert das Ergebnis entweder 1 oder 0 sein kann. Anstatt nur die Schwellenwert Funktion kann das Perzeptron auch andere Aktivierungs funktionen annehmen [figure].\n",
    "\n",
    "Folgend eine Tabelle, welche die Unterschiede aufzeigt:\n",
    "\n",
    "| Beschreibung | MP Neuron | Perzeptron |\n",
    "|---------|---------------------|----------------------|\n",
    "| Typ des Modells | Binär | Linear |\n",
    "| Schwellenwert | Statisch | Anpassbar während des Trainings |\n",
    "| Ausgabe | Binär (1 oder 0) | Kontinuierliche Werte |\n",
    "\n",
    "![MP-Neuron](images/Perceptron.drawio.svg) [todo add weights to image]\n",
    "\n",
    "\n",
    "[todo multi layer perzeptron]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation ist eine Methode welche in künstlichen Neuronalen netzwerken benutzt wird um die Gewichte von einem neuronalen Netzwerk anzupassen um vorhersagen basierend auf einem Datenset liefern zu können. Der Algorithmus betrachtet zuerst die Gewichte an der Ausgabeschicht und geht von dort aus bis hin zur Eingabeschicht. \n",
    "\n",
    "Angenommen man hat zwei Neuronen $x_1$ und $x_2$ in der Eingabeschicht, zwei Neuronen $h_1$ und $h_2$ in der versteckten Schicht, $y$ als Ausgangsneuron und die Gewichte $w1, w2, w3, w4, w5 \\text{ und } w6$ zwischen den Neuronen.\n",
    "\n",
    "Dabei berechnet man die Ausgabe $\\hat{y}$ folgendermaßen:\n",
    "\n",
    "$\\hat{y} = f(w1 * x1 + w2 * x2 + w3 * h1 + w4 * h2)$\n",
    "\n",
    "$f$ ist die Aktivierungsfunktion.\n",
    "\n",
    "Um die Werte der einzelnen Gewichte so anzupassen genauere Ergebnisse zu erhalten verwendet man Backpropagation. Bei einem Durchlauf kommt es zu dem Ergebnis $\\hat{y}$. Das ist die tatsächliche Ausgabe eines neuronalen Netzwerkes nach einem Durchlauf der Daten. Die tatsächliche Ausgabe wird mit dem Wert der erwarteten Ausgabe verglichen. Man berechnet den Fehler $\\delta$ dieser zwei Werte. \n",
    "\n",
    "$\\delta = (y - \\hat{y})^2$ \n",
    "\n",
    "Für den Fehler ist es möglich verschiedene mathematische Funktionen zu benutzen. Eine übliche Funktion ist die der mittleren quadratischen Abweichung.\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$\n",
    "\n",
    "Die Funktion quadriert jedes einzelne Fehlersignal und summt am Ende alles zusammen und teilt es durch die Anzahl der Datenpunkte. Das Ergebnis ist der durchschnittliche Fehler aller quadrierten Datenpunkte. Es wird quadriert, weil die Ergebnisse sich dadurch leichter weiterverarbeiten lassen.\n",
    "\n",
    "Nachdem das Fehler signal berechnet wurde besteht der nächste Schritt darin diesen Fehler rückwärts durch das Netzwerk zurück zu propagieren, um damit die Gewichte $w1, w2, w3, w4, w5 \\text{ und } w6$ zu erneuern. \n",
    "\n",
    "Zuerst berechnet man den Gradient $\\nabla$ von der Verlustfunktion [todo define Verlustfunktion is the loss function == error signal?] mit respekt zu jedem einzelnen Gewicht im Netzwerk. Das passiert mit der Kettenregel von der Infinitesimalrechnung. Diese macht es möglich die Ableitung von der Verlustfunktion mit Respekt zu dem Gewicht zu berechnen.\n",
    "\n",
    "Mit der Verlustfunktion $MSE$ würde der Gradient für das erste Gewicht so aussehen:\n",
    "\n",
    "$\\frac{\\partial MSE}{\\partial w1} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i}) \\cdot \\frac{\\partial \\hat{y_i}}{\\partial w1}$\n",
    "\n",
    "$\\frac{\\partial \\hat{y_i}}{\\partial w1} = \\frac{\\partial f(w1 * x1 + w2 * x2 + w3 * h1 + w4 * h2)}{\\partial w1}$\n",
    "\n",
    "$\\frac{\\partial f(w1 * x1 + w2 * x2 + w3 * h1 + w4 * h2)}{\\partial w1} = x_1 \\cdot \\frac{\\partial f}{\\partial w1}$\n",
    "\n",
    "$\\frac{\\partial MSE}{\\partial w1} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i}) \\cdot x_1 \\cdot \\frac{\\partial f}{\\partial w1}$\n",
    "\n",
    "Dieser Gradient wird nun für jedes Gewicht berechnet. Eine Formel, die die Gewichte erneuert nennt man optimierer. Es gibt verschiedene Optimierer. Der folgende ist der SGD [todo deutsch ganz]. Der SGD angewandt auf das erste Gewicht sieht so aus:\n",
    "\n",
    "$w1' = w1 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w1}$\n",
    "\n",
    "dabei ist $\\alpha$ die Lernrate. Die Lernrate ist ein Hyperparameter der die Schrittweite bestimmt in welcher Größe Gewichte erneurte werden. Bei einem kleineren $\\alpha$ sind die erneuerungen kleiner, aber genauer und bei einem größeren $\\alpha$ Wert sind die erneuerungen größer aber ungenauer. Hyperparameter nennt man die Parameter die vor einem Training eingestellt werden und nicht lernbar sind.\n",
    "\n",
    "Der SGD Optimierer wird für jedes Gewicht angewandt. \n",
    "\n",
    "$w2' = w2 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w2}$\n",
    "\n",
    "$w3' = w3 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w3}$\n",
    "\n",
    "$w4' = w4 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w4}$\n",
    "\n",
    "$w5' = w5 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w5}$\n",
    "\n",
    "$w6' = w6 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w6}$\n",
    "\n",
    "Sobald jedes Gewicht neu berechnet wurde das Netzwerk hat eine Epoche des Trainings vervollständigt. Dieser Prozess kann dann in weiteren Epochen wiederholt werden bis die Leistung des Netzwerkes zufriedenstellend ist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning ist die weiterentwicklung der vorherig beschriebenen einfachen neuralen netzwerken und ein Teilbereich des  maschinellen lernens. \n",
    "\n",
    "[todo bild mit einordnung verschiedener Bereiche]\n",
    "\n",
    "Anstatt von einer Schicht, welche die Eingabewerte von Neuronen verarbeitet gibt es bei einem Deep Learning Modell mehrere Schichten. Die genaue Anzahl an Layern, die benötigt werden um ein Modell ein Deep Learning werden zu lassen ist nicht definiert. Der Begriff Deep Learning ist also ein eher allgemeiner Begriff der Modelle beschreibt die mehrere Schichten enthalten. Eine Schicht besteht jeweils aus Eingabewerten, einer Funktion welche die Eingabewerte verarbeitet und Ausgabewerte erzeugt. \n",
    "\n",
    "[todo bild mit deep learning model mit mehreren Schichten]\n",
    "\n",
    "Es gibt verschiedene Architekturen, die für verschiedene Arten des lernens von Daten eingsetzt werden. Beispiele für Architekturen neuronaler Netzwerke sind auf folgender Grafik gut einsehbar.\n",
    "\n",
    "[link to image from https://www.asimovinstitute.org/neural-network-zoo/]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processsing (NLP) ist ein Teilbereich der Linguistik und des maschinellen lernens. Das verarbeiten von menschlicher Sprache findet eine breite Anwendung in verschiedensten Bereichen. Beispiele sind Entitäten Erkennung, Maschinenübersetzung, Spracherkennung, Sentiment Analyse. Dadurch, die sinnvolle weiterverarbeitung von Sprache eine komplexe Aufgabe ist, dadurch Wörter in verschiedene Kontexten zum Beispiel unterschiedlich Deutung besitzen können muss ein Deep Learning Modell mit viel Daten trainiert werden um den verschiedenen Kontext zu erkennen.\n",
    "\n",
    "Beispiel für unterschiedliche Wortbedeutung in unterschiedlichen Zusammenhängen:\n",
    "\n",
    "Neben der Kirche befindet sich eine Bank.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bei NLP kommt es oft zu der Notwendigkeit mit großen Mengen an Daten zu arbeiten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP algorithms require a lot of data in order to learn the complex and varied rules of natural language. This is because natural language is highly contextual and can be ambiguous, so the algorithms need to be exposed to a large number of examples in order to accurately identify the correct meaning of words and sentences. Additionally, the algorithms used in NLP are typically machine learning models, which are a type of algorithm that can improve their performance on a specific task by being trained on large amounts of data. Therefore, the more data these algorithms are trained on, the more accurate they are likely to be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
