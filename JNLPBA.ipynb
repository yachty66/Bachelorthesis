{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import evaluate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40292c5f4fb94c6285457a53160fbbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jnlpba = load_dataset('jnlpba')\n",
    "full_names_tags = {\"O\": \"Outside\", \"B-DNA\": \"Gene Begin\", \"I-DNA\": \"Gene Inside\", \"B-PROTEIN\": \"Protein Begin\", \"I-PROTEIN\": \"Protein Inside\", \"B-CELL-LINE\": \"Cell Line Begin\", \"I-CELL-LINE\": \"Cell Line Inside\", \"B-CELL-TYPE\": \"Cell Type Begin\", \"I-CELL-TYPE\": \"Cell Type Inside\", \"B-DISEASE\": \"Disease Begin\", \"I-DISEASE\": \"Disease Inside\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfind out and write down which scores I wanna meassure\\n    Loss: 0.1167\\n    Precision: 0.8225\\n    Recall: 0.8782\\n    F1: 0.8494\\n    Accuracy: 0.9621\\nloading validation dataset \\nfind out how to validate a model with validation data\\nsave results\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chintagunta85/electramed-small-JNLPBA-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"chintagunta85/electramed-small-JNLPBA-ner\")\n",
    "\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "nlp(\"Apple est créée le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société le 3 janvier 1977 à l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.\")\n",
    "\n",
    "# Convert the model to an instance of the AutoModelForTokenClassification class\n",
    "#model = AutoModelForTokenClassification.from_pretrained(model)\n",
    "\n",
    "#now i need to do evaluation\n",
    "'''\n",
    "find out and write down which scores I wanna meassure\n",
    "    Loss: 0.1167\n",
    "    Precision: 0.8225\n",
    "    Recall: 0.8782\n",
    "    F1: 0.8494\n",
    "    Accuracy: 0.9621\n",
    "loading validation dataset \n",
    "find out how to validate a model with validation data\n",
    "save results\n",
    "'''\n",
    "# Load the validation data\n",
    "#validation_data = jnlpba[\"validation\"]\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "#eval_results = model.evaluate(validation_data)\n",
    "\n",
    "# Print the evaluation results\n",
    "#print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jnlpba (/Users/maxhager/.cache/huggingface/datasets/jnlpba/jnlpba/1.0.0/3062f220823930cffde7976b694aa67bac3b06c322a02ced92d3761519810ce4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f51f6a0498940698fb602ca827ee082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_9 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_10 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/maxhager/.virtualenvs/thesis/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "task_evaluator = evaluator(\"token-classification\")\n",
    "data = load_dataset(\"jnlpba\", split=\"validation\")\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=\"chintagunta85/electramed-small-JNLPBA-ner\",\n",
    "    data=data,\n",
    "    metric=\"seqeval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation[:2]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(jnlpba[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation[:2]\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/datasets/dataset_dict.py:57\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, k) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, (\u001b[39mstr\u001b[39m, NamedSplit)) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(k)\n\u001b[1;32m     58\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m             split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     61\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation[:2]'"
     ]
    }
   ],
   "source": [
    "print(jnlpba[\"validation[:2]\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model on which I want to fine tune on\n",
    "# i go the dataset page and than I go to the leaderbord and see a model on the first place \n",
    "# i take this model because the leaderboard shows which model has the best scores on the dataset and i want to be state of the art\n",
    "#SOTA model for NER on jnlpba is KeBioLM and achieves F1 82\n",
    "\n",
    "#I just realize that the model is possible to evaluate like its written here https://huggingface.co/sciarrilli/biobert-base-cased-v1.2-finetuned-ner#:~:text=it%20achieves%20the%20following%20results%20on%20the%20evaluation%20set%3A\n",
    "#there is now absolute no need to fine tune the model. but it could be a good introduction into doing finetuning. I basically can just copy the variables which the guy displays\n",
    "#can do the fine tuning with this variables - yes lets do it!\n",
    "\n",
    "#--> found new sota for token classification https://paperswithcode.com/sota/token-classification-on-jnlpba\n",
    "#here https://huggingface.co/chintagunta85/electramed-small-JNLPBA-ner?text=My+name+is+Wolfgang+and+I+live+in+Berlin on huggingface\n",
    "#was fine tuned on https://huggingface.co/giacomomiolo/electramed_small_scivocab\n",
    "#the reason why i wanna fine tune is because i want to figure out how this works for my seq2seq later\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
