{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. split pandas dataframe on \\x000 value -\\n2. iter over each line - \\n3. check if attribute appears from second position in first part of line - \\n4. if no drop this line - \\n4. store this df in a separate dataframe\\n6. if yes substitute attribute with value from second part of line\\n7. tokenize each line \\n8. create a json file with df from step 4 with sentence and tokens\\n\\n\\n1. \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. split pandas dataframe on \\0000 value -\n",
    "2. iter over each line - \n",
    "3. check if attribute appears from second position in first part of line - \n",
    "4. if no drop this line - \n",
    "4. store this df in a separate dataframe\n",
    "6. if yes substitute attribute with value from second part of line\n",
    "7. tokenize each line \n",
    "8. create a json file with df from step 4 with sentence and tokens\n",
    "\n",
    "\n",
    "1. \n",
    "'''\n",
    "# TODO \\tSOTEK-JSWX-XHZ-72v20ah2000w Electric bicycle                     nan,SOTEK,72v   shouldnt be in the dataset\n",
    "# TODO deal with null labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from IPython.display import display\n",
    "#pd.options.display.max_colwidth = 100\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0           1    2\n",
      "0  APG STO0045  Camping Stove Portable Cooking Eq...        Fuel  Gas\n",
      "1  APG STO0045  Camping Stove Portable Cooking Eq...  Brand Name  APG\n",
      "2  APG STO0045  Camping Stove Portable Cooking Eq...       Model  NaN\n",
      "3  APG STO0045  Camping Stove Portable Cooking Eq...  Disposable  NaN\n",
      "4  80LUX Mini Portable Camping Lantern Gas Light ...    Material  NaN\n"
     ]
    }
   ],
   "source": [
    "# load data from data/paper folder\n",
    "data = pd.read_csv('data/ACL19.txt', sep='\\u0001', header=None)\n",
    "print(data.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fuel' 'Brand Name' 'Model' ... 'Fitness' 'Applicable Parts' 'Anti-skid']\n"
     ]
    }
   ],
   "source": [
    "#get all values from colum 1 so that they uniquely appear in a dictionary with token\n",
    "attributes = data[1].unique()\n",
    "\n",
    "print(attributes)\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    for i in attributes:\n",
    "        f.write(i + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>\\t2018 New Men&amp;#39;s Basketball Shoes High Top Breathable Sneakers Ankel Boots Athletic Tenis Jordan Basketball Shoes big size shoes</th>\n",
       "      <td>Athletic Shoe Type,Shoe Width,Feature,Gender</td>\n",
       "      <td>Basketball Shoes,nan,nan,Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\tHUISON S4 Professional Carbon Fiber Zebra Tec wood Table Tennis paddle/ Table Tennis Blade/ table tennis bat</th>\n",
       "      <td>Model Number,Size</td>\n",
       "      <td>S4,nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\tJordan 9 XII Men Basketball Shoes wool the master University Blue gym red GS Barons Flu Game Athletic Outdoor Sport Sneakers..</th>\n",
       "      <td>Athletic Shoe Type</td>\n",
       "      <td>Basketball Shoes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               1  \\\n",
       "0                                                                                                  \n",
       "\\t2018 New Men&#39;s Basketball Shoes High Top ...  Athletic Shoe Type,Shoe Width,Feature,Gender   \n",
       "\\tHUISON S4 Professional Carbon Fiber Zebra Tec...                             Model Number,Size   \n",
       "\\tJordan 9 XII Men Basketball Shoes wool the ma...                            Athletic Shoe Type   \n",
       "\n",
       "                                                                               2  \n",
       "0                                                                                 \n",
       "\\t2018 New Men&#39;s Basketball Shoes High Top ...  Basketball Shoes,nan,nan,Men  \n",
       "\\tHUISON S4 Professional Carbon Fiber Zebra Tec...                        S4,nan  \n",
       "\\tJordan 9 XII Men Basketball Shoes wool the ma...              Basketball Shoes  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groupby same first column\n",
    "data = data.groupby(0).apply(lambda x: pd.Series({\"1\": \",\".join(x[1].astype(str)), \"2\": \",\".join(x[2].astype(str))}))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nis column 1 now a string only? i can get all the unique values before i groupby everything. \\n\\n1. Get all unique values from column one\\n2. Create a set like tag_set = {\\n    \"PERSON\": 1,\\n    \"ORGANIZATION\": 2,\\n    \"LOCATION\": 3,\\n    ...\\n}\\n3. split values from column 1&2 at the comma and make them to lists\\n1. iter over each line \\n2. if only nan types in colum 2 than drop this line\\nx. iter over list of colum 2 and check for each item if it appears in colum 0. if yes than replace the appearance in colum 0 with the item from colum 1 which appears on the same position like the item in colum 2\\n    additionally check if the replacement is contained in a another string if yes than seperate the item inside of the string with spaces \\n4. i need to convert each word to tokens now but first check all the attributes look like   \\n\\nProblem with Soccer gender and Soccer gender: because if I have a string with Soccer gender: and i check if Soccer gender appears than it labels it as Soccer gender but its normally Soccer gender:. In this example its not wild because its anyway the same label but if you have some like gender soccer and gender than its a problem because borh are completely wrong labels.\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "is column 1 now a string only? i can get all the unique values before i groupby everything. \n",
    "\n",
    "1. Get all unique values from column one\n",
    "2. Create a set like tag_set = {\n",
    "    \"PERSON\": 1,\n",
    "    \"ORGANIZATION\": 2,\n",
    "    \"LOCATION\": 3,\n",
    "    ...\n",
    "}\n",
    "3. split values from column 1&2 at the comma and make them to lists\n",
    "1. iter over each line \n",
    "2. if only nan types in colum 2 than drop this line\n",
    "x. iter over list of colum 2 and check for each item if it appears in colum 0. if yes than replace the appearance in colum 0 with the item from colum 1 which appears on the same position like the item in colum 2\n",
    "    additionally check if the replacement is contained in a another string if yes than seperate the item inside of the string with spaces \n",
    "4. i need to convert each word to tokens now but first check all the attributes look like   \n",
    "\n",
    "Problem with Soccer gender and Soccer gender: because if I have a string with Soccer gender: and i check if Soccer gender appears than it labels it as Soccer gender but its normally Soccer gender:. In this example its not wild because its anyway the same label but if you have some like gender soccer and gender than its a problem because borh are completely wrong labels.\n",
    "To avoid the gender / gender soccer proplem I could say - check first the longest strings and if they do not appear in the string go to the smaller string. soccer gender soccer - would check firt with gender soccer and than directly convert gender soccer into. \n",
    "\n",
    "People from the paper implemented it different. They did the following:\n",
    "- removing all rows with NULL \n",
    "i still have critics over the current process because \n",
    "what about I write in my introduction that I deal with two cases. In one case i have given a dataset and make this intentionally weak and perform various models on it and than I take a dataset which is given\n",
    "\n",
    "Than I can write my introduction like \"Die Arbeit behandelt zwei Datensets. Ein Datenset bei dem NER\"\n",
    "\n",
    "In the end I want to show ways on how to do NER with weakly labelled data. My idea would be the following:\n",
    "\n",
    "- [ ] taking the bio dataset and doing basic NER on it\n",
    "\n",
    "I need certain steps which I reproduce for both datasets. for example ner, ner with weakly labelled data, seq2seq, \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[39mreturn\u001b[39;00m row \n\u001b[1;32m      9\u001b[0m \u001b[39m# now i need to apply this method to each line and if the method returns False than drop this line from the df else replace row with returnt row               \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m data_new \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mapply(process_row, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39m#result from above should be a df where some rows are == False\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[39m#remove rows where value == False\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data_new \u001b[39m=\u001b[39m data_new[data_new \u001b[39m!=\u001b[39m \u001b[39mFalse\u001b[39;00m]\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_row\u001b[39m(row):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(row[\u001b[39m2\u001b[39;49m]) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(row[\u001b[39m0\u001b[39m]):\n\u001b[1;32m      3\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/pandas/core/series.py:978\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    975\u001b[0m     key \u001b[39m=\u001b[39m unpack_1tuple(key)\n\u001b[1;32m    977\u001b[0m \u001b[39mif\u001b[39;00m is_integer(key) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m--> 978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m    981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "def process_row(row):\n",
    "    if str(row[2]) not in str(row[0]):\n",
    "        return False\n",
    "    else:\n",
    "        if str(row[2]) in str(row[0]):\n",
    "            row = row[0].replace(str(row[2]), str(row[1]))\n",
    "            return row \n",
    "        \n",
    "# now i need to apply this method to each line and if the method returns False than drop this line from the df else replace row with returnt row               \n",
    "data_new = data.apply(process_row, axis=1)\n",
    "#result from above should be a df where some rows are == False\n",
    "\n",
    "#remove rows where value == False\n",
    "data_new = data_new[data_new != False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    APG STO0045  Camping Stove Portable Cooking Eq...\n",
      "1    Brand Name STO0045  Camping Stove Portable Coo...\n",
      "5    Model Card furnace Windproof Portable Stoves G...\n",
      "6    BDZ-160-A Card furnace Windproof Portable Stov...\n",
      "7    BDZ-160-A Card furnace Windproof Portable Stov...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print first element in first line of df\n",
    "print(data_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'bag_of_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [129], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m data_new \u001b[39m=\u001b[39m data_new\u001b[39m.\u001b[39mapply(nltk\u001b[39m.\u001b[39mword_tokenize)\n\u001b[1;32m     15\u001b[0m \u001b[39m#apply bag of words\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m data_new \u001b[39m=\u001b[39m data_new\u001b[39m.\u001b[39mapply(nltk\u001b[39m.\u001b[39;49mbag_of_words)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'bag_of_words'"
     ]
    }
   ],
   "source": [
    "#tokenize dataframe\n",
    "#nltk.download('all')\n",
    "#what i want basically is to take all my words and than to turn each word into a number\n",
    "#than i can iterate over the df and turn a word which is not in the attributes list into a zero\n",
    "#if a word is in the list than i can substitute this word with \n",
    "\n",
    "\n",
    "#is there no method for turning each word from a dataset into a number were the same words getting the same number?\n",
    "#Which method is used for labelling a dataset for named entity recognition?\n",
    "#\n",
    "data_new = data_new.apply(nltk.word_tokenize)\n",
    "\n",
    "#apply bag of words\n",
    "data_new = data_new.apply(nltk.bag_of_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(APG, NNP), (STO0045, NNP), (Camping, NNP), (...\n",
      "1    [(Brand, NNP), (Name, NNP), (STO0045, NNP), (C...\n",
      "5    [(Model, NNP), (Card, NNP), (furnace, NN), (Wi...\n",
      "6    [(BDZ-160-A, NNP), (Card, NNP), (furnace, NN),...\n",
      "7    [(BDZ-160-A, NNP), (Card, NNP), (furnace, NN),...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data_new.head())\n",
    "#turn into number tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(APG, NNP), (STO0045, NNP), (Camping, NNP), (...\n",
      "1    [(Brand, NNP), (Name, NNP), (STO0045, NNP), (C...\n",
      "5    [(Model, NNP), (Card, NNP), (furnace, NN), (Wi...\n",
      "6    [(BDZ-160-A, NNP), (Card, NNP), (furnace, NN),...\n",
      "7    [(BDZ-160-A, NNP), (Card, NNP), (furnace, NN),...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
